[["index.html", "Arts escèniques A proposit del treball", " Arts escèniques Projecte de pràctiques d’empresa del grau F.P. ASISX I.E. Maria Enriquez Gandia 2022 Versio web i altres formats en https://incitato.github.io Ramiro Palau Gregori 2022-06-12 A proposit del treball Documentació dels projectes proposats a realitzar en el desenvolupament de les pràctiques realitzades en Espacio Inestable. Aquest manual servira de base per desenvolupar posteriorment un mes detallat per deixar-lo per als propers administradors de la sala.. El llibre encara està en fase de proves, igual que part de la documentació. Mes be, es una recopilació de procediments per implementar serveis, i apunts per resoldre problemes. El projecte està en fase de proves d’aprenentatge dels recursos a implantar, a punt de passar a una verdadera simulació. Es va fer una primera fase en Virtual Box, per provar els serveis i elegir els que millor s’adaptaven al propòsit del projecte. Una vegada triats, vam buscar un hypervisor de tipus 1, passant a estudiar com fer-los funcionar en el nou entorn. Les alternatives van ser Xen i Proxmox, de codi lliure, elegint finalment Proxmox. La GUI per configurar-lo no ten limitacions com les que vaig trobar per a Xen, la majoria trial 30 dies, o versions incompletes community. Vam visualitzar en KVM, ja que la base de la host és una ubuntu, i encara que Proxmox està basat en Debian, i existeix un paquet .deb per instal·lar en aquesta distribució, no és compatible en Ubuntu directament. En la tercera fase, s’instal·lara directament en Debian, i tindrem accés directe a les targetes de xarxa, no fent ponts linux a KVM i d’aci a les VM, i als discs o particions, no havent de fer volums virtuals per simular els verdaders. "],["propòsit.html", "Propòsit", " Propòsit L’empresa vol desenvolupar els seguents serveis. Implantar un sistema d’accés a internet wifi per al públic que entre en les instal·lacions. Implantar un sistema de gravació de les obres que es representen per reproducció en continu o tindre una còpia per després editar-les. Un entorn col·laboratiu per als empleats. Es proposa la següent solució Instala.lacio de dos punts d’accés wifi d’alta capacitat UniFi. Una xarxa i sistema cctv de gravació de video amb càmeres IP Un entorn de treball col·laboratiu tipus Nextcloud, que inclou serveis de missatgeria, entorn ofimàtic… Un servidor per allotjar les VM dels serveis i el NAS d’emmagatzemament dels recursos centralitzat. Instal·lacio d’una xarxa per donar accés als recursos. En els propers capitols s’anirà detallant la forma de portar-ho a terme. Plan B En els capitols següents s’estudia la forma d’implementar-ho perquè siga en un futur més fàcilment escalable. El projecte ha d’estar acabat en september, i s’està a l’espera que siguen concedides les subvencions. Si correra presa per posar-lo en funcionament, directament es faria en Truenas, com a base del sistema, ja que té la capacitat de visualitzar, disposa d’alguns dels serveis com Nextcloud com a connector. Instal·lem en un Jail, i ja ho tenim configurat. La política de grups i usuaris, es pot fer des del mateix Truenas, ames del firewall, i OpneVPN. Seria una xarxa molt més simple, inclòs, no fent VPN i directament traure fora, port forwarding, els ports de Nextcloud i Zonemindre. Un RAID 1 per a les dades, per tindre redundància. Com pensava que tenia temps, craso error, m’he entretingut complicar-me la vida. "],["agraïments.html", "Agraïments", " Agraïments Agraïments als tutors: Pablo Navarro Mon Jacobo Pallares Burriel Ulisses Alonso Pels ànims, la paciència i llibertat que m’han deixat per desenvolupar el projecte. Sin noticias de Gurb (Eduardo Mendoza) - ¿qué alternativa le ves? - Bueno, pues quedarnos en éste. - ¿Y hacer qué? - Uf, mogollón de cosas. - Como por ejemplo qué. - Montamos un bar tú y yo. - Mira qué bien: "],["infraestructura.html", "Capitol 1 Infraestructura", " Capitol 1 Infraestructura Definiren la distribució dels espais en el local de l’empresa i la configuració de la xarxa a muntar, per donar els serveis requerits. "],["xarxa-per-a-la-sala.html", "1.1 Xarxa per a la sala", " 1.1 Xarxa per a la sala Es proposa la següent configuració de xarxa per a la sala. Xarxa de Video per al registrament / reproducció en línia de les obres de teatre. Xarxa de wifi, pública i privada. Xarxa Privada per als treballadors, amb connexió externa per VPN. Xarxa d’administració "],["descripció-de-les-installacions.html", "1.2 Descripció de les instal·lacions", " 1.2 Descripció de les instal·lacions Pla de la sala Pla de la sala La sala la podem dividir en 4 espais Oficina Escenari Entrada Rebosts, en un d’ells posarem el Rack de comunicació i servidor. 1.2.1 Oficina En l’oficina tenim l’entrada de la connexió de fibra a internet. Des d’aci portem la connexió de xarxa al rack de comunicacions on tenim la infraestructura de la xarxa, el commutador i el servidor. Des del rack de comunicacions, tornarem connexions de xarxa cap a l’oficina per donar accés als serveis interns VLAN 20, per poder fer l’edició de video i la reproducció en línia si es donara el cas VLAN 30, i una toma de xarxa per realitzar l’administració VLAN 1. Per facilitar el seu ús, cadascuna d’aquestes per cables separats acabant en una roseta etiquetada segons VLAN. 1.2.2 Sala d’espera En aquest espai, tenim l’ordinador de venda d’entrades al que li donarem connexió, siga directa al commutador, per la connexió de l’oficina o el canal wifi privant. En el centre de la sala aniria l’antena wifi 2 tipus PoE connectada al nucli de comunicacions per on tindrem VLAN 20 30 separades en dos SSID Privat i Pública. També posarem una connexió VLAN 30 per si en un futur, es vol reciclar alguna camera per fer de videovigilància. Font a aquesta connexió hi ha un projector al qual li podem posar una roseta amb la VLAN 20 per reproduir arxius de video editats o presentacions des de VLAN 20 (no està dibuixada per claredat de l’esquema). 1.2.3 Escenari En la sala d’espectacles es traurien 4 connexions de la xarxa video VLAN 30. Central, baix de la taula de llums, on es connectara un Camera ip PoE fixa. Lateral, una a cada banda de l’escenari, camera desmuntable que es pot traslladar lateralment segons les necessitats de l’obra representada. Es recomana que aquesta siga del tipus PTZ. Podent ser col·locada a la dreta o l’esquerra de l’escenari. Una toma en la taula de llums, per poder retransmetre el video en la mateixa representació. Una connexió per l’antena wifi, per poder ser sevir el movil com a camera web, amb app tipus IP webcam, Irun webcam. Micròfon ambient que el connectarem a la camera central que es fixa. Una connexió per a la xarxa pública VLAN 40, per l’antena wifi, al centre del sostre de l’escenari, per donar millor cobertura. Un punt de connexió en el vestuari a la VLAN 20, per comunicació amb la xarxa interna, que pot donar cobertura per cable a l’escenari en cas de conferències, o altres actes que necessiten bona connexió de xarxa. Un altre punt de connexió a aquesta VLAN 20 en la taula de llums, per tindre accés als recursos interns i poder fer comunicació pel TALK de Nextcloud amb els vestuaris i oficines. 1.2.4 Rebost En un dels rebosts que hi ha en la sala, muntarem el Rack de comunicacions, aprofitarem en el que està instal·lada la infraestructura de luminació del escenari, que disposa de bones connexions elèctriques, bon aïllat acustica i tèrmicament. "],["descripció-de-les-xarxes.html", "1.3 Descripció de les xarxes", " 1.3 Descripció de les xarxes La configuració de les xarxes, la farem amb 4 VLAN. 1.3.1 VLAN 1 Administració És la xarxa per on s’executaran les tasques d’administració dels serveis amb ssh, i per on passara tot el tràfic intern entre les diferents VM, autenticació LDAP, recursos d’espai de les aplicacions. Aquesta xarxa, interna del servidor per open v Switch, i soles tindrà una eixida física a lo’oficina. I una OpenVPN pròpia per gestionar des de l’exterior, amb accés soles per a l’administrador. 1.3.2 VLAN 20 Privat Xarxa amb connexió a internet i als serveis oferits per la sala. Es donaran els següents accessos. Taula de llums, perquè puguen accedir a recursos guardats en el servidor, mantenir comunicació amb el vestuari. Vestuaris, comunicació o recursos de xarxa a l’escenari. Oficines, donar accés a recursos del servidor, donar xarxa a telèfon ip instal·lat. Punts wifi, crearem una segona xarxa amb enquesta Vlan en les antenes wifi de sol accés per als treballadors de la sala. 1.3.3 VLAN 30 Video Xarxa per la qual anirà el circuit intern de televisió, siga per la reproducció en línia o per fer gravació de les obres en el servidor. No disposara de connexió a internet. Configurarem accés a aquesta Vlan des del punt wifi de l’escenari, per poder gastar el movil com a camera en les representacions. En la taula de llums, si volen fer servir aquest servei, no volen tindre cap connexió a internet en l’ordinador que gestionen les llums i el video, per no tindre sorpreses de notificacions, correus … En meitat d’una representació. Per poder guardar el tràfic de video d’aquesta xarxa, tenim diverses opcions Muntar en el servidor una LXC de Zonaminder, Shinobi… on gestionar les gravacions de video de les càmeres. Amb aquest tipus de programes NVR es facilita molt la gestió aquesta xarxa. La gravadora (NVR) En un sistema de càmeres de seguretat IP, el NVR (Network Video Recorder) és central que gestiona i emmagatzema les imatges de vídeo. Totes les càmeres IP enviaran les seues dades a les gravadores des de les quals podeu visualitzar en directe o fer còpies de seguretat de les gravacions. Les càmeres IP es poden gestionar des del seu propi servidor web, on podem configurar l’espai d’emmagatzemament per FTP, SMB … Depenent del protocol que utilitzen, es pot compartir l’espai de video de Truenas amb el mateix protocol i programar-ho des de qualsevol connexió VLAN 30. Des de l’oficina utilitzant programes tipus OSB per fer la reproducció en línia o copia a l’espai de Truenas Haguérem d’afegir el servei Truenas a aquesta xarxa VLAN 30 en el cas 2, i a la VLAN 20 en el cas 3, que en principi no està previst, es planteja fer tot el tràfic de dades per la VLAN 1. 1.3.4 VLAN 40 pública Xarxa amb connexió a internet per al públic general a través dels punts wifi, i totalment aïllada de les altres VLAN. La podem configurar amb un portal captiu, amb pfSense, amb permís de connectar a certes hores coincidint en les representacions, canviant el password en cada obra, que oferirem al públic per un codi QR. Posar un sistema de tiquets que per a les connexions … Es pot aprofitar aquesta connexió per a redirigir als espectadors en un primer moment a un servidor web propi on publicitar ressenyes de properes obres, donar més informació de l’obra que van a veure, o interactuant amb ells via web en el transcurs de la representació. 1.3.5 Xarxa interna dels serveis El disseny per a la xarxa de cada VM el representem en el següent diagrama, seria afegir les targetes necessaries per a cada VM i connectar-les a les VLAN que necessitem. Esquema de les xarxes Tots els serveis administratius: Compartició d’espai d’emmagatzemament, autenticació LDAP, còpies de seguretat i administració dels serveis, aniran per la VLAN 1. Tots les VM tindran una connexió en aquesta VLAN. Els serveis per al personal de l’empresa i convidats: Nextcloud i Zoneminder correran per la VLAN 20 que tindrà eixida a l’exterior per les antenes wifi i VPN. Aquesta xarxa sols oferira serveis per HTTPS 443, en un futur es podria plantejar oferir més serveis com correu, SMB, FTP. Pero amb l’entorn Nextcloud, els podem centralitzar amb aquesta aplicació. La xarxa de video VLAN 30, principalment correra per cable, no disposa de connexió a internet. L’entrada per la xarxa wifi, sols es farà servir per connectar mòbils per fer de web cam en els espectacles, no sent tan fiable aquest tipus de connexió per a la retransmissió de video d’alta qualitat. En els punts de connexió d’aquesta VLAN , taula de llums i oficina, es podrà connectar directament a les càmeres, per poder editar les imatges per altres programes tipus OBS per fer streaming, o retransmetre al mateix espectacle.en directe. Per acabar, la VLAN 40 estarà aïllada de la resta de xarxes pel firewall de pfSense, Soles sera servida pels punts wifi, La forma de gestionar aquests punts d’accés queda oberta a l’elecció del’empresa. Un password que es proporcionara per codi QR, un sistema de portal captiu, sistema de tiquets… La millor opció seria el portal captiu i reencaminar després a un servidor web intern per aquesta xarxa amb promocions de les obres. Aquest servidor web s’implementaria amb un contenidor, ja siga Wordpress, apache amb Pico de fulles estàtiques, en la carpeta conectada, on deixem els fitxers en md per generar les pàgines, estarà compartit amb nextcloud recurs extern, per poder generar-les des d’alli. "],["proposta-de-projecte.html", "Capitol 2 Proposta de projecte", " Capitol 2 Proposta de projecte Requeriments de l’empresa Implantar un sistema d’accés a internet wifi per al públic que entre en les instal·lacions. Implantar un sistema de gravació de les obres que es representen per reproducció en continu o tindre una còpia per després editar-les. Un entorn col·laboratiu per als empleats. "],["serveis-proposats.html", "2.1 Serveis proposats", " 2.1 Serveis proposats Entorn de serveis d’allotjament de fitxers Nextcloud, és funcionalment similar a Dropbox, Office 365 o Google Drive quan s’utilitza amb les seues solucions d’oficina integrades Collabora Online o OnlyOffice. Servei de comunicació entre membres, per Nextcloud Talk, video, audio o xat. Servei de wifi per a la sala, separant tres xarxes. Interna, membres de l’equip (internet i xarxa local de serveis) VLAN 20 Públic (sols accessos a internet) VLAN 40 video (sense accés a internet) VLAN 30 serveis de video, tipus videovigilància, que ens permet gravar les obres, retransmetre a usuaris autenticats en directe (per veure assajos), veure còpies d’esdeveniments gravats. La forma més senzilla és fent servir solucions NVR tipus Zoneminder o shinobi. Aprofitant la wifi pública, habilitar un servidor web intern en aquesta xarxa per promocions, documentació extra de les obres, o interactuar amb el públic. Per portar-lo a terme es requeriria Muntar punts wifi d’alta capacitat. Un servidor per allotjar les VM dels serveis, espai per guardar els documents i les gravacions de video. Instal·lacio d’una xarxa informàtica, commutador, cablejat, Rack. "],["pressupost-del-maquinari.html", "2.2 Pressupost del maquinari", " 2.2 Pressupost del maquinari Es proposa el següent maquinari Servidor Cost 250 euros + iva -&gt; 300 euros server No porta Discs durs.{width=25%} Discs durs per al servidor, minim 3 per fer un sistema de seguretat a fallades, i un SSD per al sistema base i les VM. Disposa d’espai per a 4 de connexió en calent dins la seua caixa, si fera falta més, en un futur es posaran, pero ja fora del xassís. Cost de 4TB, al voltant de 90 euros cada un Referències orientatives SEAGATE Barracuda ST4000DM004 4000GB 3.5 Serial ATA III amazon Barracuda 4TB Switch, referencies orientatives Es requereix que siga PoE+ (alimentació elèctrica del dispositiu pel cable LAN) per a les càmeres IP i les dues antenes wifi. De 24 ports. El model s’elegira segons presupost final. cost entre 300 i 600 Armari Rack per al servidor i el switch. Cost Microconnect Armario Rack 19” 6U 600X600mm Negro 130,56€ Patch panel Cost depen dels ports del switch, al voltant de 30 euros Cablejat i rosetes, Eas contractara una empresa externa especialitzada. Antena wifi Unifi U6 Professional $ 149.00 U6 Lite $ 99.00 Càmeres ip. En la sala coneixen una persona que treballa en el món d’imatge i so, esperant recomanacions. Professional Sony, pero de 2500 euros no baixen. Barates càmeres amb nightcolor barata 67.99 euros Reolink 5MP PTZ Cámara 139,99€ fixa reolink 4K 94,99€ Segons fabricant La tecnologia d’avantguarda NightColor us permet obtenir una imatge nítida fins i tot quan és fosc. Aquesta càmera té 2 LED blancs que transmeten els seus raigs infrarojos en una freqüència imperceptible a l’ull humà. Les llums blanques emeten brillants que ofereixen una increïble gamma de NightColor de 66 peus. Amb aquesta potent funció, podeu aconseguir una consciència total sobre allò que no s’havia vist anteriorment i fer-ho d’una manera indetectable. Pero no se jo. Es pot provar en una a veure com va, si no dona bon resultat es pot posar de camera de vigilància de l’entrada de la sala. Un poc millor Amcrest UltraHD 4K (8 MP) POE IP, càmera exterior, 3840 x 2160, 131 pies NightVision 129,99€ especificacions Gamma Mitjana Zowietek PTZ Pro Cámara PTZ Al voltant de 700 euros D’aquesta gamma hi ha moltes. "],["servidor.html", "Capitol 3 Servidor", " Capitol 3 Servidor Servidor. En ebai el podem trobar per uns 250 euros + IVA, amb 64G de ram. serverR320 Especificacions server Documentació del model, i manuals, owner manual Compatibilitat amb ubuntu server, és compatible amb la versió 14 i posteriors. En la fulla de Dell És compatible amb la versió 14.04 LTS, no apareix en la llista, manté la compatibilitat hardware amb les més recents. És un servidor del 2014, pero per als requisits que requerim, pocs usuaris, tràfic limitat, és suficient. A una mala Posem CentOS Dimensions servidor dimensions servidor Cal buscar un rack que done les dimensions. video d’instal·lació Discs durs, admet segons documentació 4 SATA 3.5’’ o 8 de 2.5’’ Hot-plug. No fariem RAID per hardware. RAID: Kit RAID H710 Mini 512 MB NV (SAS/ SATA ) - 0/1/5/6/10/50/60 Caddies: 4 LFF (3.5”) incluidos hd i Raid Recomane SEAGATE Barracuda ST4000DM004 4000GB 3.5 Serial ATA III amazon amazon usa Farien falta, 3 SATA per les dades, i un ssd 2.5 per al sistema base i les VM. Estaria bé tindre un de reserva per si falla un disc, no haver d’anar buscant un altre igual. ` RAM: RAM registrada DDR3 de 64 GB. No és la més ràpida del mercat, pero suficient per al que necessitem: pfSense Min 1GB (imatge oficial), 2GB Znemindre NVR video 4 GB, si sobra, posar un poc mes, com anem a fer registre directe, sense detecció de moviment, no requereix tants recursos. Nextcloud Min 2GB, recomanat + 8GB Ldap, web server, antenes wifi, Collabora office server, algun altre servei futur, si es vol mail intern … 8GB. Truenas, minim 8GB, quant mes millor. Les exigencies de ZFA son altes. CPU: 1x Intel Xeon E5-2440 V2: 8 núcleos, 16 subprocesos, 1,90 GHz (aumento de 2,40 GHz, caché de 20 MB, TDP de 95 W) Segons la informació del venedor. Bisel: No incluido Rieles: Rieles no incluidos Factor de forma: Montaje en rack 1U Part posterior: 1 puerto RJ-45, Preguntar si no és uno doble. En la documentacio oficial, la sèrie porta I/O adapter options 1Gb Ethernet: Broadcom 5720 Dual Port 1Gb NIC The Broadcom 5720 is a 14th generation 10/100/1000Base-T Ethernet LAN controller Broadcom 5720 2x1Gb Base-T 2 fonts d’alimentació redundants. Fuente de alimentación: 2 fuentes de alimentación intercambiables en caliente de Dell Platinum efficiency 350W or 550W power supply Dell OpenManage Systems Management OpenManage Essentials Acustica, sobre 30db. El posarem en una habitació fora de l’oficina i la sala. No sera problema. "],["proxmox.html", "Capitol 4 Proxmox", " Capitol 4 Proxmox Instal·larem el hypervisor Proxmox Ho farem en proves viralitzant en KVM en ubuntu local, en el servidor sera directe. Per a poder importar les imatges de les iso o VM que volem instal·lar crearem un servei NFS en el host local per a poder recuperar-les i no haver d’anar en USB. "],["preliminars.html", "4.1 Preliminars", " 4.1 Preliminars 4.1.1 Servidor NFS en local Servirem les imatges de les ISO des de el host local per no tindreles que torna a baixar. Ho farem servint un directori NFS al Guest Proxmox. 4.1.1.1 En el host local, instal·lem i arranquem el servei Seguint la guia, primer hem d’arrancar el servei sudo apt install nfs-kernel-server Creem la carpeta on compartirem les imatges i VM per a proxmox, canviem el propietari i els permisos. sudo mkdir nfsdir sudo chown nobody:nogroup /home/enkidu/nfs sudo chmod 777 /home/enkidu/nfs Ara exportem el recurs, editem el fitxer /etc/exports i afegim la linea per donar permisos d’accés als clients. NFS iso i VM Exportem el directori i reiniciem el servei sudo exportfs -a sudo systemctl restart nfs-kernel-server Ja tenim preparat el directori de les imatges per quan acabem d’instal·lar Proxmox "],["installacio-de-proxmox-en-una-vm-amb-kvm.html", "4.2 Instal·lacio de Proxmox en una VM amb KVM", " 4.2 Instal·lacio de Proxmox en una VM amb KVM Ho farem en KVM des de l’Ubuntu local Creem la VM Elegim el medi d’instal·lacio, ho fem des d’un fitxer iso, en el servidor real, tindríem que haber fet un USB d’arrancada d’aquesta iso seguint els passos que ens indique. Crear VM en qemu Elegim la imatge i li diguen quina es la base del SO, Debian 10 Elegir el fitxer iso Creem el volum per aquesta VM, el format és qcow2 amb espai dinàmic, i la mida 40 Gib, després li afegirem un segon volum on posarem les VM. En disc ocupa 2 Gib reals, /mnt/vbox és on munte una partició del ssd en el meu portàtil, servirem d’aquest. Volum Al final queda la VM com la imatge, després ja li afegirem altres HD i targetes de xara. Li he donat quasi tota la RAM, reservant part per a la host. VM configuració inicial "],["arranquem-la-installacio.html", "4.3 Arranquem la instal·lacio", " 4.3 Arranquem la instal·lacio En arrancar la VM ens ix la presentació presentacio proxmox Elegim el format del HD, el podem deixar en ext4, he provat fer ZFS, per fer la prova, en el cas de tindre el servidor, es pot plantejar fer-ho amb 2 ssd RAID 1. Pero amb una bona política de còpies de seguretat, no seria necessari. Una matriu de paritat com ara RAID (1, 5, 6 o 10) introdueix cicles de treball estés i acumulació de calor reduint així la vida útil del motor. Una matriu de striping com ara RAID-0 redueix els cicles de treball i l’acumulació de calor augmentant així la vida útil del motor. També ofereix un major rendiment com a avantatge, mes velocitat de lectura i escriptura. configuracio HD Configuracio HD Després configurem la zona horària i el password, Per entrar des d’un navegador ip port 8006 amb usuari root i el password que acabem de posar. passwrd Configurem la xarxa cap a fora, internet NAT al host. Donem el FQON que sere proxmox.inestable.dedyn.io La ip 192.168.122.2/24 El Gateway 192.168.122.1 DNS 192.168.122.1 Ho canviarem després Triarem X.X.X.254 en pfSense tant per a la passarel·la com per a DNS perquè es troba fora dels valors predeterminats de l’interval d’adreces DHCP (192.168.x.100-192.168.x.200) a la majoria d’encaminadors. Directrius RFC 1918. Al final queda una configuració configuracio final Canviar la Timezone després :) En acabar es reinicia i passem a la configuració Final instal·lacio "],["primeres-configuracions.html", "4.4 Primeres configuracions", " 4.4 Primeres configuracions Anem en el navegador a l’adreça IP seleccionada, al port 8006, i amb l’usuari root i el password que hem elegit, entrem. proxmox gui I ja estem dins en la primera arrencada Proxmox primera arrancada Muntem el volum NFS d’abans, on tenim les ISO de les VM que volem instal.lar Entrem per ssh en proxmox, també ho podem fer des de la GUI en la finestra del navegador. enkidu@enkidu:/mnt/vbox$ ssh root@192.168.122.2 root@192.168.122.2&#39;s password: Linux proxmox 5.15.30-2-pve #1 SMP PVE 5.15.30-3 (Fri, 22 Apr 2022 18:08:27 +0200) x86_64 The programs included with the Debian GNU/Linux system are free software; the exact distribution terms for each program are described in the individual files in /usr/share/doc/*/copyright. Debian GNU/Linux comes with ABSOLUTELY NO WARRANTY, to the extent permitted by applicable law. Last login: Sun May 22 22:42:06 2022 from 192.168.122.1 root@proxmox:~# Busquem el recurs nfs on estan les ISO, la meua host te la ip 192.168.0.103 Creem un punt per enllaçar el recurs i el muntem. root@proxmox:/mnt# pvesm nfsscan 192.168.0.103 /home/enkidu/nfs 192.168.122.2 root@proxmox:~# mkdir -p /mnt/pvex/enkidu root@proxmox:~# mount -t nfs 192.168.0.103:/home/enkidu/nfs /mnt/pvex/enkidu root@proxmox:~# ls -la /mnt/pvex/enlidu total 5 drwxrwxrwx 2 nobody nogroup 4096 May 22 19:54 . drwxrwxrwx 4 nobody nogroup 4 May 22 22:57 .. drwxrwxrwx 1 nobady nogroup 46 May 22 19:54 isos_esceniques drwxrwxrwx 1 nobody nogroup 32 May 22 19:53 vm Ho podem fer també per la GUI de proxmox. En vista de l’emmagatzemament -&gt; emmagatzematge -&gt; NFS Des d’aci és on es configura els tipus de discs que podem crear o afegir. Gui emmagatzematge Ara configurem el nou recurs. Muntar NFS Li donem un ID, que sera també el nom de la carpeta que es crea en /mnt/vpex/per muntar el recurs. La IP o FQON on està el recurs. Com encara no tenim DNS, li posem la IP. El directori compartit, al posar la IP, en el menu desplegable ens mostra els recursos compartits per aquest servidor NFS. El tipus de recurs que es comparteix. Volem guardar aci les imatges ISO, podem seleccionar altres recursos com copies de seguretat, Discs VM … Afegim I ja tindríem el recurs accessible. Recurs NFS Ja podríem començar a crear les VM en el volum local dins la VM, pero primer afegirem un nou HD, per tindre separat el sistema host de les VM, es com si ara li posarem un altre HD ssd al servidor, ho recomana la documentació oficial per més estabilitat. En la pràctica, segurament tindrem un sol ssd per al proxmox i les VM. Faríem les particions abans una per al sistema base i l’altra per a les VM, en el cas de tindre dos SSD, faríem un RAID1 (espill) + ZFS, i el mateix una partició separada de l’arrel per a posar les VM. En gestió personalitzada dels dics. Per fer el RAID1 no el faríem des del maquinari del servidor el servidor no es nou, i si fallara, haguérem de buscar la mateixa peça perquè arranque, el format ZFS no es compatible amb RAID per maquinari, ja porta el seu propi sistema de fer-ho. "],["actualitzar-repositoris.html", "4.5 Actualitzar repositoris", " 4.5 Actualitzar repositoris Per defecte els repositoris que porta definits es la versió enterprise, que s’ha de tindre subscriptcio per poder accedir, canciarem per la No-Subscription Repository Documentacio En /etc/apt/source.list afegim i el repositori No-Subscription commentem l’enterprise deb http://ftp.debian.org/debian bullseye main contrib deb http://ftp.debian.org/debian bullseye-updates main contrib # PVE pve-no-subscription repository provided by proxmox.com, # NOT recommended for production use deb http://download.proxmox.com/debian/pve bullseye pve-no-subscription # security updates deb http://security.debian.org/debian-security bullseye-security main contrib "],["afegir-nous-hd-al-servidor.html", "4.6 Afegir nous HD al servidor", " 4.6 Afegir nous HD al servidor Ara seria el moment que afegiríem els HD al servidor físic. El cas hipotètic seria tindre 3 HD SATA i un ssd. Tots els discs es formataran fent servir ZFS pels avantatges que proporciona, instantànies, control d’errors, redimensionament de volums, compressió nativa … El que es proposa El SSD siga d’us exclusiu de Proxmox, una partició per al sistema base i l’altra per guardar les VM dels serveis. I els SATA per ús exclusiu de Truenas, i des d’aci compartir volums en la resta de VM, tenint centralitzada l’emmagatzemament de dades facilitant les còpies de seguretat. Els SATA es dividiran en dos blocs, SATA 1 i 2 format un RAID 0 que ens proporciona millor rendiment de velocitat, on crearem espai compartit per NFS per: Dades d’usuaris Nextcloud. Còpies de seguretat de les VM, fitxers de configuració, repositori ISO dels sistemes a instal·lar, part administrativa. Espai per la captura de video, en dues seccions, una per a videos editats per guardar, i l’altra per a registres en brut. Altres serveis futurs, com un xicotet espai per al servidor web de promocions internes, que podria estar dins de l’espai de Nextcloud per poder editar des d’aques entorn el contingut (configuració amb pico), o espai per contingut Wordpress si s’adopta aquesta opció. L’altre SATA per fer còpies de seguretat. De les dades de Nextcloud. De les còpies de seguretat de Proxmox i fitxers administratius. De la carpeta de Video de còpies per guardar. Soles els arxius de video d’obres pròpies. Per la forma en què l’empresa vol d’utilització les còpies de video, fer captures de video per a les companyies convidades per oferir-les com un servei mes de la sala, aquestes còpies s’esborraran una vegada el convidat la descarregue. I les pròpies s’han d’editar. No val la pena estar gastant gran quantitat de recursos del servidor en fer un RAID 1, quan el pes més gran de les dades son descartables. Amb una política frequent rsync entre volums en Truenas, les tindrem protegides amb poc consum. Discs en el servidor 4.6.1 Procediment Simularem que li afegim un nou SSD de 100Gib i tres SATA dos de 50 Gib i un de 100 Gib. Creem els 4 volums en format qcow2 i els afegim en KVM a la VM de Proxmox En KVM/qemu creem els discs i els afegim a la nostra VM. Proxmox afegir SSD per a VM Proxmox SATA Afegim els discs, com a VirtiO. Ho farem primer soles en el SSD i després posarem els altres. Proxmox afegir SSD Arranquem. i apareix el nou HD /dev/vdb SSD 4.6.2 Configuracio del nou SSD per a les VM Tenim dues opcions, configurar el nou volum com a LVM-thin o com un volum ZFS Si el volguérem configurar com a LVM Documentacio root@proxmox:~# cat /proc/partitions major minor #blocks name 252 0 41943040 vda 252 1 1007 vda1 252 2 524288 vda2 252 3 41417711 vda3 252 16 104857600 vdb 11 0 1048575 sr0 Crearem un LVM per al nou SSD que acabem d’instal·lar, aquesta opció te tots els avantatges de LVM, podem afegir mes espai si en el futur ens fera falta, afegint un nou ssd, canviar la mida de les VM … Per consola, primer crear el volum, soles tenim un i després el grup de volums que li direm lvsVM es per a imatges de màquines virtuals. root@proxmox:~# pvcreate /dev/vdb Physical volume &quot;/dev/vdb&quot; successfully created. root@proxmox:~# vgcreate lvmVM /dev/vdb Volume group &quot;lvmVM&quot; successfully created També ho podem fer per la GUI, els mateixos passos que farem després en el format ZFS pero elegint LVM Afegim el volum logic root@proxmox:~# lvcreate -n VM -V 100G lvmVM/lvmVM WARNING: Sum of all thin volume sizes (100.00 GiB) exceeds the size of thin pool lvmVM/lvmVM and the size of whole volume group (&lt;100.00 GiB). WARNING: You have not turned on protection against thin pools running out of space. WARNING: Set activation/thin_pool_autoextend_threshold below 100 to trigger automatic extension of thin pools before they get full. Logical volume &quot;VM&quot; created. En aquest tipus de volums l’hauríem de configurar com a LVM-thin si el volem utilitzar per a emmagatzemament d’imatges, ofereix un suport eficient per a instantànies i clons. És la forma predeterminada de particions de Proxmos per a les VM que genera en el volum local, fa una part per al sistema base i una altra que li diu local per a les VM, és el que es veu en el disc /dev/vda. Com a volum ZFS Al final el creem com a ZFS, que té les mateixes característiques que LVM-thin millorades. Dins de discs elegim ZFS -&gt; afegir, seleccionem el disc que volem crear, apareixen els nous discs afegits. El /dev/vde és el ssd de 100G que configurarem. Crear disc VM El crearia i l’afegeix com a nou emmagatzemament El podem editar i li diguem que tipus de contingut volem que es guarde en ell, Imatges de disc i contenidors. vmZFS per VM i contenidors Ja el podríem utilitzar per crear la primera VM i guardar-la dins d’ell. "],["afegir-lemmagatzenament-de-còpia-de-seguretat.html", "4.7 Afegir l’emmagatzenament de còpia de seguretat", " 4.7 Afegir l’emmagatzenament de còpia de seguretat Una vegada creat i compartit el volum de còpia de seguretat, en la secció de Truenas, l’afegim a emmagatzemaments. Aquest volum l’hem compartit com a NFS + ZFS, ho fem com en el cas del disc de la host -&gt; afegir NFS, pero ara li direm que guarde en tipus de dades, elegim totes. Aci guardarem tant les ISO, les VM, els contenidors que baixem… sera d’on en un futur obtindrem els recursos d’administració, eliminant l’espai compartit de la host d’on importem ara les ISO per instal·lar les VM. Afegir HD NFS Backup Ja tindríem el nou espai backup "],["crear-nova-vm.html", "4.8 Crear nova VM", " 4.8 Crear nova VM Crearem una nova VM Truenas per definir els passos a seguir. Creem una nova VM, elegim el node, soles tenim un proxmox L’ID, es com reconeix Proxmox la VM Li donem un nom Nova VM truenas Elegim el SO, li diguem el lloc on està la imatge, de moment, del recurs compartit en la host, més avant seria el recurs Backup d’administracio en Truenas. Elegim de les ISO disponibles la que volem Elegim el tipus de sistema a instal·lar, Linux Nova VM truenas Podríem elegir targeta gràfica, en la que ve per defecte ens va be, Si la VM requerira potència GPU i el servidor disposara, la podríem configurar. El tipus d’emulacio del procesador. Nova VM truenas Definim el tipus de disc SATA On es creara la VM, en l’espai que hem configurat per aquesta tasca vmZFS La grandària del disc El format, per caracteristique del lloc on creem les VM, soles ens deixa elegir el format RAW, pero en ser ZFS i l’hem definit la compressio, és millor que un volum qcow2, té accés directe al disc va més fluid, i mantenim la mida dinamica. Nova VM truenas Configurariem el número de CPU Nova VM truenas La memoria, Elegim la maxima que pot utilitzar Per la forma de gestionar la memoria, li podem definir la minima, si no l’utilitza, no reserva tota la que li hem donat, soles la minima, alliberant la resta al sistema. Comparticions, crec recordar que és un mòdul del nucli linux, que comparteix memoria amb altres VM, aquesta és una FreeBSD, les llibreries igual amb una altra FreeBSD les comparteixen, alliberant memoria. Nova VM truenas La Xarxa, de moment per a les primeres configuracions la posem en el pont vmbr0, és el pont NAT a la host, després editarem la configuracio de la VM i la posarem en el pont Open virtual switch, en les VLAN que volem que estiga, afegint-li més targetes de xarxa. Nova VM truenas Tenim el resum de la VM que acabem de crear, Soles quedaria arrancar-la i comença el proces d’instal·lacio, en l’apartat Truenas. Nova VM truenas Imatge 4.1: Instal·lacio VM Truenas "],["contenidors-lxc-en-proxmox.html", "4.9 Contenidors LXC en Proxmox", " 4.9 Contenidors LXC en Proxmox Els contenidors són una alternativa lleugera a les màquines totalment virtualitzades (VM). Utilitzen el nucli del sistema amfitrió en què s’executen, en lloc d’emular un sistema operatiu (SO) complet. Els costos d’execució dels contenidors són baixos, normalment insignificants. Tanmateix, hi ha alguns inconvenients que cal tenir en compte: Només es poden executar distribucions de Linux als contenidors Proxmox. Per motius de seguretat, s’ha de restringir l’accés als recursos de l’amfitrió. Per tant, els contenidors s’executen en els seus propis espais de noms separats. Proxmox VE fa servir Linux Containers (LXC) com a tecnologia de contenidors subjacent. Es gestiona amb “Proxmox Container Toolkit” ( pct ). Els contenidors estan estretament integrats amb Proxmox VE, podem utilitzar la mateixa xarxa i recursos d’emmagatzematge que les màquines virtuals. Si voleu executar contenidors d’aplicacions, per exemple, imatges Docker, es recomana que els executeu dins d’una VM Proxmox Qemu. 4.9.1 Distribucions suportades Basicament les més importants, algunes d’elles són Alpine Linux Arch Linux CentOS Debian Ubuntu … 4.9.2 Imatges de contenidors Les imatges de contenidors, “templates” són arxius tar que contenen tot per executar un contenidor. Proxmox VE proporciona una varietat de plantilles bàsiques per a les distribucions de Linux més comunes. Es poden descarregar mitjançant la GUI o la utilitat de línia d’ordres pveam, també es poden descarregar plantilles de contenidors TurnKey Linux. root@proxmox:~# pveam update update successful root@proxmox:~# pveam available mail proxmox-mailgateway-6.4-standard_6.4-1_amd64.tar.gz mail proxmox-mailgateway-7.0-standard_7.0-1_amd64.tar.gz system almalinux-8-default_20210928_amd64.tar.xz system alpine-3.12-default_20200823_amd64.tar.xz system alpine-3.13-default_20210419_amd64.tar.xz system alpine-3.14-default_20210623_amd64.tar.xz system alpine-3.15-default_20211202_amd64.tar.xz system archlinux-base_20211202-1_amd64.tar.zst system centos-7-default_20190926_amd64.tar.xz system centos-8-default_20201210_amd64.tar.xz system centos-8-stream-default_20220327_amd64.tar.xz system debian-10-standard_10.7-1_amd64.tar.gz system debian-11-standard_11.3-1_amd64.tar.zst system devuan-3.0-standard_3.0_amd64.tar.gz system devuan-4.0-standard_4.0_amd64.tar.gz system fedora-34-default_20 ... Abans de poder usar aquesta plantilla, cal que la descarregueu a un dels volums emmagatzematges. En el nostre cas en el volum backup que es on guardem les iso, VM i ara les imatges dels contenidors. Baixarem una Bebian 11, on després farem servir par Zoneminder. root@proxmox:~# pveam download backup debian-11-standard_11.3-1_amd64.tar.zst downloading http://download.proxmox.com/images/system\\ /debian-11-standard_11.3-1_amd64.tar.zst to\\ /mnt/pve/backup/template/cache/debian-11-standard_11.3-1_amd64.tar.zst --2022-05-29 23:10:05-- http://download.proxmox.com/images/system/debian-11-standard_11.3-1_amd64.tar.zst Resolving download.proxmox.com\\ (download.proxmox.com)... 51.91.38.34, 2607:5300:203:7dc2::162 Connecting to download.proxmox.com (download.proxmox.com)|51.91.38.34|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 123216040 (118M) [application/octet-stream] Saving to: &#39;/mnt/pve/backup/template/cache/debian-11-standard_11.3-1_amd64.tar.zst.tmp.195914&#39; 0K ........ ........ ........ ........ 27% 800K 1m49s 32768K ........ ........ ........ ........ 54% 709K 73s 65536K ........ ........ ........ ........ 81% 517K 34s 98304K ........ ........ ..... 100% 765K=2m59s 2022-05-29 23:13:05 (671 KB/s) - &#39;/mnt/pve/backup/template/cache/debian-11-standard_11.3-1_amd64.tar.zst.tmp.195914&#39; saved [123216040/123216040] calculating checksum...OK, checksum verified download of &#39;http://download.proxmox.com/images/system/debian-11-standard_11.3-1_amd64.tar.zst&#39; to &#39;/mnt/pve/backup/template/cache/debian-11-standard_11.3-1_amd64.tar.zst&#39; finished Una vegada ja la tenim, procedirem a fer la instal·lacio d’un contenidor amb aquesta plantilla. Que utilitzarem per al servei de captura de video. 4.9.3 Creem una CT amb la imatge que acabem de baixar Afegir contenidor És molt paregut a la creacio de VM, pero en aquest cas hem de donar un password per al root del contenidor, i tindre en compte de marcar contenidor unprivileged. Contenidors sense privilegis Els contenidors sense privilegis fan servir una nova característica del nucli anomenada espais de noms d’usuari. L’UID arrel 0 dins del contenidor està assignat a un usuari sense privilegis fora del contenidor. Això vol dir que la majoria dels problemes de seguretat (escapada de contenidors, ús abusiu de recursos, etc.) en aquests contenidors afectaran un usuari aleatori sense privilegis, i seria un error de seguretat genèric del nucli més que un problema de LXC. L’equip de LXC creu que els contenidors no privilegiats són segurs per disseny. En l’apartat de disks, podem afegir nous discs i seleccionant el punt on es munta en mount points, en l’exemple en /mnt/espai. Es pot fer servir despres per muntar el volum de video de truenas, o fer-ho per NFS. Punt de muntage Punts de muntatge amb suport d’emmagatzematge Els punts de muntatge amb suport d’emmagatzematge els gestiona el subsistema d’emmagatzematge Proxmox VE i es presenten en tres tipus diferents: Basades en imatges: són imatges en brut que contenen un únic sistema de fitxers amb format ext4. Subvolums ZFS: tècnicament són muntatges d’enllaç, però amb emmagatzematge gestionat i, per tant, permeten redimensionar i capturar instantànies. Directoris: passar size=0 activa un cas especial en què es crea un directori en lloc d’una imatge en brut. Per poder gestionar-lo en shell, camand pct pct start 100 # arrancar pct console 100 # inici de sessio pct enter 100 # Shell amb root ... Imatge 4.2: (Contenidor LXC) Tambe el podem arrancar per la GUI, com cualsevol altra VM Contenedor amb la GUI El primer que hem de fer es un upgrede i update. I ja està a punt per a gastar. Provem si té IP asignada. root@Zoneminder:~# ip a 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0@if33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 link/ether 9e:ab:81:76:34:b7 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 192.168.122.250/24 brd 192.168.122.255 scope global dynamic eth0 valid_lft 3416sec preferred_lft 3416sec inet6 fe80::9cab:81ff:fe76:34b7/64 scope link valid_lft forever preferred_lft forever root@Zoneminder:~# "],["subxarxes-en-proxmox.html", "4.10 Subxarxes en Proxmox", " 4.10 Subxarxes en Proxmox Documentacio oficial Open vswitch PfSense virtualitzat a Proxmox amb Open vSwitch Crearem la xarxa interna per a les VM la forma mes facil és per Open vSwitch, té una lògica més neta que el pont Linux i està dissenyat específicament per funcionar en entorns virtualitzats. En la consola de Proxmox instal·lem openvswitch-switch, no s’instal·la de manera predeterminada. apt update apt install openvswitch-switch Es recomana que el pont estiga lligat a un port troncal sense vlans sense etiquetar; això vol dir que el nostre pont mai tindrà una adreça IP. Dividim les nostres VLAN etiquetades mitjançant interfícies virtuals (OVSIntPort) per si necessitem accedir a aquestes VLAN des de la nostra host local. Proxmox assignarà a les màquines virtuals convidades una interfície de toc associada a una vlan, de manera que no necessitem un pont per vlan Per dividir les vlans amb ips per utilitzar-les a l’amfitrió local, hauríem d’utilitzar OVSIntPorts Perquè l’amfitrió (per exemple, l’amfitrió proxmox, no les màquines virtuals, utilitze una vlan dins del pont, heu de crear OVSIntPorts. Per a VLAN 1 que dona flux d’espai d’emmagatzenament.) Aquests OVSintPorts que creeu també han d’aparéixer a la definició del pont real a ovs_ports. Si no ho fan, no es mostraran encara que hàgeu especificat un ovs_bridge. La diferència principal és que en lloc de tidre un pont per vlan, tenim un únic pont que conté totes les nostres vlan. Aleshores, quan configureu la interfície de xarxa per a la màquina virtual, seleccionareu el pont nou pont OVS i li assignem una VLAN. Quedaria xarxa OVS El pont vmbr0 és un pont linux, és el que es crea automaticament en instal·lar Proxmox. Que està connectat a enp1s0, que és la interfase que ell creu real del servidor (dispositiu de xarxa). Aquest pont té la IP 192.168.122.2/24 i gateway 192.168.122.1 és per on entra internet NAT des de la host. No és convenient tindre xarxes linux i xarxer OVS barretjades. Hauriem d’eliminar aquest pont i connectar le enp1s0 al nou pont OVS vmbr1 directament, de moment he preferit no tocar res, ja que molts problemes que he tingut en les xarxes és per la configuracio de la prova. Virtualitzar el proxmox. En el cas real, o en la seguent fase de les proves, el Proxmox anira directament al hardware de la màquina, sense ser virtualitzat. Com de moment funciona, no fare més proves per aquest cami, que no adelenta res en el resultat final. El vmbr1 es el pont OVS que hem creat per a la xarxa interna de les VM, es tan sencill com anar a xarxes i crea nou pont OVS Pont OVS En aquest pont OVS, no li donem IP ni gateway (pren la predeterminada del sistema, la definida en vmBr0). En font li diguem les interfices que estan connectades, la intovs1 (és la interfase internade proxmox que es connecta a la xarxa NAT), i les VLAN 1 20 30 40 que són les interfases internas de proxmox per a les VLAN que crearem a continuacio. Són per a què Proxmox és conecta a aquestes VLAN, la veritat és que soles ens faria falta la VLAN 1 que és per on comparteix l’espai de backup truenas. Pont OVS En el cas posterio que eliminem el pont vmbr0, si li tindriem que donar la IP i el gateway, com els que te aquest. Pont linux vmBr0 Per crear una OVS intport, igual que hem fet en el pont pero ara elegim intPort Li donem un nom, li he posat VLAN 1, haguera sigut millor dir-li TeagetaVlan1 Li donem IP estatica La vinculem al pont OVS vmbr1 IntPort per a VLAN 1 de proxmox Quedaria per a cada VM maquinari -&gt; afegir -&gt; dispositiu de xarxa, crear una interfase de xarxa nova, asociarla al pont OVS i seleccionar la VLAN a la qual es connecta. Xarxa Truenas nova interfase Veiem que tenim net1 al pont vmbr0, està l’eliminariem, una vegada tot configurat, és la que he utilitzat per fer les configuracions des del navegadordel host local per comoditat. En el cas de pfSense, no li afegim una targeta de xarxa per a cada VLAN. Li afegim soles una sense definir la VLAN, i OVS la tacta com a Trunk, per on van totes les xarxes. Des de dins de pfSense, es creen les diferents interfases per a cada VLAN. Fem un ping a la IP de pfSense en la VLAN 1 des de Proxmox per comprovar que esten en la VLAN 1 root@proxmox:~# ping 172.16.0.254 PING 172.16.0.254 (172.16.0.254) 56(84) bytes of data. 64 bytes from 172.16.0.254: icmp_seq=1 ttl=64 time=5.79 ms 64 bytes from 172.16.0.254: icmp_seq=2 ttl=64 time=2.70 ms --- 172.16.0.254 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1002ms rtt min/avg/max/mdev = 2.698/4.245/5.792/1.547 ms 4.10.1 Segon intercace, post install En el cas real, o en la seguent fase de les proves aquest apartat no és necesari, és soles per poder traure la xarxa interna a l’exterior, queda per acabar de configurar. La primera la crea automàticament i la tenim en NAT, es la que anirà connectada directament l’encaminador que ens dona internet. Farem un pont en el host per simular l’altra interfices del servidor, una anirà pel primer pont en NAT, i l’altra la connectem a enp3s0 que és l’ethernet física del portàtil, per connectar aquesta a un encaminador casolà que farà de switch, o el switch físic real. Documentacio pont linux El dimoni libvirtd s’està executant, es crea una xarxa per defecte. Podem comprovar que aquesta xarxa existeix mitjançant la virsh $ sudo virsh net-list --all Name State Autostart Persistent -------------------------------------------- default active yes yes La seua configuració es sudo virsh net-edit default &lt;network&gt; &lt;name&gt;default&lt;/name&gt; &lt;uuid&gt;4aee1ff6-80c4-4edb-b155-7abd67ca293a&lt;/uuid&gt; &lt;forward mode=&#39;nat&#39;/&gt; &lt;bridge name=&#39;virbr0&#39; stp=&#39;on&#39; delay=&#39;0&#39;/&gt; &lt;mac address=&#39;52:54:00:d5:0b:34&#39;/&gt; &lt;ip address=&#39;192.168.122.1&#39; netmask=&#39;255.255.255.0&#39;&gt; &lt;dhcp&gt; &lt;range start=&#39;192.168.122.2&#39; end=&#39;192.168.122.254&#39;/&gt; &lt;/dhcp&gt; &lt;/ip&gt; &lt;/network&gt; virbr0 utilitza connectivitat basada en NAT per connectar les màquines virtuals que formen part de la xarxa amb el món exterior. Tenint activat servei DHCP. Per veure qui esta connectat aquest pont $ ip link show master virbr0 16: vnet5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master virbr0 state UNKNOWN mode DEFAULT group default qlen 1000 link/ether fe:54:00:df:dd:09 brd ff:ff:ff:ff:ff:ff Soles tenim connectada la vnet5, que apareix en arrancar la màquina virtual, El que volem es una connexió de pont completa, on els dispositius convidats estan connectats a la LAN amfitrió, sense fer servir NAT, hauríem de crear un nou pont i compartir una de les interfícies físiques d’Ethernet de l’amfitrió. Primer creem un nou pont anomenat br0 i mostrem els ports $ sudo ip link add br0 type bridge $ sudo ip link show type bridge 8: virbr0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000 link/ether 52:54:00:d5:0b:34 brd ff:ff:ff:ff:ff:ff 17: br0: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000 link/ether 82:bd:1e:40:5c:fd brd ff:ff:ff:ff:ff:ff Li afegirem la interfície física host enp3s0, faig servir la de xarxa, perquè per a la connectivitat faig gastar la wifi, si connectàrem aquesta, perdríem la connectivitat, ja que perdria la seua IP. Primer l’alcem i l’afegim al pont $ sudo ip link set enp3s0 up $ sudo ip link set enp3s0 master br0 RTNETLINK answers: Operation not supported De moment no va, queda pendent resoldre-ho, provarem una altra via, macvtap macvtap és per connectar interfícies de contenidors directament amb interfícies d’amfitrió. Provar que ho soporta el nucli $ sudo modprobe macvlan lsmod | grep macvlan Fer el fitxer xml on definim el pont macvtap &lt;network&gt; &lt;name&gt;macvtap-net&lt;/name&gt; &lt;forward mode=&quot;bridge&quot;&gt; &lt;interface dev=&quot;enp3s0&quot;/&gt; &lt;/forward&gt; &lt;/network&gt; Per enllaçar directament a enp3s0 Feu servir l’virsh net-defineordre amb aquest XML per definir la xarxa Libvirt real. El fitxer anterior està en macvtap_enp3s0.xml virsh net-define macvtap_enp3s0.xml Network macvtap-net defined from macvtap_enp3s0.xml A continuació, establiríem la xarxa Libvirt resultant per iniciar-la automàticament. $ sudo virsh net-autostart macvtap-net Network macvtap-net marked as autostarted $ sudo virsh net-start macvtap-net Network macvtap-net started Després la connectem a la VM proxmox segon interficei connectada directa a enp3s0 Aquest mètode si funciona, ja tenim les dues xarxes al servidor, ara hem de configurar el pfsense perquè es connecte al switch per aquesta i podríem connectar als recursos del servidor des d’un altre ordinador concertat a un router. "],["còpies-de-seguretat.html", "4.11 Còpies de seguretat", " 4.11 Còpies de seguretat Les imatges de les VM no haurien de canviar molt en el temps, per això, tindrem una politica de còpies de seguretat d’una vegada al mes, mantenint la primera configuracio blocada a l’esborrat. Durem a terme aquesta tasca amb l’opcio que de backup que porta el GUI, tenint en compte de no afegir discs durs secondaris de les VM si en tingueren, no es el cas, en la forma que ho hem configurat, l’espai per a dades el compartim per xarxa, i no entra en le backup per defecte. Per fer les còpies de seguretat de les VM anem a centre de dades En centre de dades -&gt; còpia de seguretat -&gt; afegir nova ens ix el seguent panell. Node, el nostre node, soles en tenim un. Emmagatzenament, on volem que es cree la imatge, en Backup que es l’espai compartit pel nas. Programes, configurem la frecuancia de les còpies, una al mes a les 3 a.m. Elmode, soles les escollides, ja que la de Truenas no volem que la faça. Compresio, podem elegir el tipus, que compremeix mes i tarda més, el de per defecte va bé. El mode, seleccionem si volem instantanea, suspensio o aturada, le més segura és aturada, i com les farem a altes hores i no tindra usuaris el servei, no deu ser problema, si no podera para, es faria instanatanea. Configurar la còpia de seguretat de les VM En l’opcio retention configurem perquè soles guarde les dues últimes. Seleccionar les remanents Per conservar una imatge determinada, en el nostre cas la primera de configuracio inicial, en el volum on estan guardades, backup, seleccionem la que volem protegir i li canviem en change protection que deuria d’apareixer ara l’escut. Ramanencia d’una còpia en concret Imatge 4.3: Copia seguretat VM 4.11.1 Copia de seguretat de truena No podem fer una còpia de seguretat de Truenas i guardar-la per xarxa en Truenas. En principi es podria fer una instanatanea, pero s’atura el servei i es queda clavat. Farem còpia de seguretat en local i després la pasarem al directori de Backup amb rsync. La fem com en el cas anterior, pero ara la guardarem en local. Còpia de seguretat Truenas local i li diguem que guarde les dues últimes Les guada en ./var/lib/vz/dump/vzdump-qemu-102-2022_05_27-22_27_02.vma.zst les passarem per rsync al directori de backup de les VM compartit per Tuenas, està en /mnt/pve/backup/dump root@proxmox:/mnt/pve/backup/dump# ls vzdump-qemu-105-2022_06_02-00_02_12.log vzdump-qemu-107-2022_05_30-23_54_17.log vzdump-qemu-105-2022_06_02-00_02_12.vma.zst vzdump-qemu-107-2022_05_30-23_54_17.vma.zst vzdump-qemu-105-2022_06_02-00_02_12.vma.zst.notes vzdump-qemu-107-2022_05_30-23_54_17.vma.zst.notes vzdump-qemu-107-2022_05_30-23_49_20.log root@proxmox:/mnt/pve/backup/dump# rsync -a /var/lib/vz/dump/ /mnt/pve/backup/dump Quan acaba, ja ens apareix en l’espai d’emmagatzenamet backup. Sincronitzacio de la imatge Truanas Farem un scipt perquè sincronitze el directori local amb del backup i ho afegirem a cron en la periocitat bque hem definit per a fer les còpies, en principi a principi de cada mes, deixarem un marge de temps i sincronitzarem una hora després. #!/bin/sh rsync -a /var/lib/vz/dump/ /mnt/pve/backup/dump 4.11.2 Còpia de seguretat de Proxmox Com el sistema base de proxmox, una Debian 11 està en un volum ZFS fem un clon de rpool que es un estan el directori arrel i el guardem en l’espai Backup en el servidor nas. Com estem en el porces de proves i no voldria trencar les cosesque de moment van, he efegit un altre disc virtual la VM de proxmox l’hem creat un colum ZFS que es diu vmZFS1 per fer les proves de clonacio. en backup seria igual pero encaminant al directori on està muntat vmZFS Primer mirem els pools que té proxmox root@proxmox:~# zpool status pool: rpool state: ONLINE config: NAME STATE READ WRITE CKSUM rpool ONLINE 0 0 0 vda3 ONLINE 0 0 0 errors: No known data errors pool: vmZFS state: ONLINE config: NAME STATE READ WRITE CKSUM vmZFS ONLINE 0 0 0 vdb ONLINE 0 0 0 errors: No known data errors pool: vmZFS1 state: ONLINE config: NAME STATE READ WRITE CKSUM vmZFS1 ONLINE 0 0 0 vde ONLINE 0 0 0 errors: No known data errors pool: zfspool1 state: ONLINE config: NAME STATE READ WRITE CKSUM zfspool1 ONLINE 0 0 0 vdc ONLINE 0 0 0 vdd ONLINE 0 0 0 errors: No known data errors root@proxmox:~# zfs list -r rpool NAME USED AVAIL REFER MOUNTPOINT rpool 15.2G 22.6G 96K /rpool rpool/ROOT 15.2G 22.6G 96K /rpool/ROOT rpool/ROOT/pve-1 15.2G 22.6G 15.2G / rpool/data 15 El sistema base Debian de proxmox està en rpool, i l’arrel en rpool/ROOT/pve-1. Per poder clonar, primer hem de fer un snapshot i després clonar d’aques snap segons la documentacio clones ZFS root@proxmox:/rpool# df Filesystem 1K-blocks Used Available Use% Mounted on udev 14001688 0 14001688 0% /dev tmpfs 2807172 1304 2805868 1% /run rpool/ROOT/pve-1 39619072 15897344 23721728 41% / tmpfs 14035840 43680 13992160 1% /dev/shm tmpfs 5120 0 5120 0% /run/lock vmZFS 68554496 128 68554368 1% /vmZFS rpool 23721856 128 23721728 1% /rpool rpool/data 23721856 128 23721728 1% /rpool/data rpool/ROOT 23721856 128 23721728 1% /rpool/ROOT zfspool1 13056 128 12928 1% /mnt/zfspool1 /dev/fuse 131072 20 131052 1% /etc/pve vmZFS1 101088768 128 101088640 1% /vmZFS1 172.16.0.4:/mnt/datatruenas/backup 55712384 5337856 50374528 10% /mnt/pve/backup vmZFS1/backup 101088768 128 101088640 1% /vmZFS1/backup tmpfs 2807168 0 2807168 0% /run/user/0 root@proxmox:/rpool# zfs snapshot rpool/ROOT/pve-1@today root@proxmox:/rpool# zfs list -t snapshot NAME USED AVAIL REFER MOUNTPOINT rpool@proxmox 0B - 96K - rpool@today 0B - 96K - rpool/ROOT@base_data 64K - 96K - rpool/ROOT/pve-1@today 1.72M - 15.2G - rpool/data@base_data 56K - 96K - vmZFS/base-103-disk-0@__base__ 8K - 2.69G - vmZFS/vm-100-disk-0@pfsense 310M - 1.24G - vmZFS1/backup@today 0B - 96K - root@proxmox:/rpool# zfs send rpool/ROOT/pve-1@today | zfs receive vmZFS1/backup cannot receive new filesystem stream: destination &#39;vmZFS1/backup&#39; exists must specify -F to overwrite it root@proxmox:/rpool# zfs send rpool/ROOT/pve-1@today | zfs receive vmZFS1/poxmox Ens assegurem que tenim els arxius clonats. Anem a on munta el volum /vmZFS1, i veiem que ho ha clonat. root@proxmox:/# cd vmZFS1/ root@proxmox:/vmZFS1# ls -l total 9 drwxr-xr-x 4 root root 4 May 22 21:07 backup drwxr-xr-x 21 root root 27 Jun 5 10:10 poxmox root@proxmox:/vmZFS1# cd proxmox -bash: cd: proxmox: No such file or directory root@proxmox:/vmZFS1# cd poxmox/ root@proxmox:/vmZFS1/poxmox# ls backupZFS bin boot dev etc home lib lib32 lib64 libx32 media mnt opt proc root rpool run sbin srv sys tmp usr var vmZFS vmZFS1 root@proxmox:/vmZFS1/poxmox# En el cas de perdre el sistema base, instalariem una nova còpia de Proxmox i en aquesta carpeta tindriem una còpia de l’original que accederiem simplement muntant el volum, o si tinguerem la copia en un servidor en un altre node, accedint per xarxa. Tratant-se d’una debian, en tindre una còpia de /etc/pve /etc/network/interfaces /etc. /passwd i /etc/resolv.conf seria suficient per restaurar, Pero no ve mal tindre una idea com fer clon de volums, per si es treballa en cluster poder migrar VM entre ells. Si s’adopta aquesta solucio per fer la còpia, faltaria fer un script i posar-lo en cron amb un pipe després de send per comprimir la còpia abans d’enviar-la. #!/bin/sh zfs snapshot rpool/ROOT/pve-1@today zfs send rpool/ROOT/pve-1@today | zfs receive vmZFS1/backupProxmox Un altre script per comprimir el /etc i remetre-lo al backup i posar-lo en cron. Script /etc #!/bin/sh timestamp=&quot;$(date +&#39;%b-%d-%y&#39;)&quot; tar -cvpzf /vmZFS1/etcBackup/etcBackup-${timestamp}.tar.gz /etc 4.11.3 pve-zsync Pendent d’estudiar funcionament Tambe es pot fer el rsync per la GUI de Proxmox, instal·lant pve-zsync Primer hem d’instal·lar pve-zsync apt-get install pve-zsync 4.11.3.1 Principals característiques Limitador de velocitat L’interval de sincronització es pot establir mitjançant cron Sincronització de VM (discs i configuració), però també conjunts de dades ZFS Pot mantenir diverses còpies de seguretat Es pot utilitzar en ambdues direccions Es pot enviar a l’amfitrió local El trànsit està encriptat "],["pfsense.html", "Capitol 5 pfSense", " Capitol 5 pfSense Utilitzem pfsense com a cor de la xarxa El projecte pfSense® és una distribució personalitzada gratuïta de codi obert de FreeBSD dissenyada per utilitzar-la com a tallafoc i encaminador, gestionada completament per una interfície web fàcil d’utilitzar, el programari pfSense inclou una llarga llista de funcions relacionades. El sistema de paquets pfSense permet una major expansió sense afegir vulnerabilitats de seguretat potencials a la distribució base. "],["per-a-què-el-farem-servir.html", "5.1 Per a què el farem servir", " 5.1 Per a què el farem servir Tallafocs perimetrals, pfSense admet xarxes que requereixen diverses connexions a Internet, diverses xarxes LAN i diverses xarxes DMZ. De moment soles configurarem una WAN, pero estarà preparat pel seu escalat si fera falta en un futur, donar més amplada de banda o redundància de connexió a internet. També per a un servidor web en la zona DMZ per als espectadors en la xarxa pública. Encaminador LAN, per connectar diversos segments de xarxa interna amb VLAN configurades amb troncal 802.1Q. Dispositiu VPN, com a dispositiu de xarxa privada virtual independent afegeix capacitats VPN sense interrompre la infraestructura del tallafoc existent i inclou diversos protocols VPN. Configurarem OpenVPN per als treballadors i convidats. Dispositiu de servidor DHCP, permet que un dispositiu com el programari pfSense® assigne dinàmicament adreces IP als clients des d’un grup d’adreces predefinits. DHCP també envia informació de configuració als clients, com ara una passarel·la, servidors DNS, nom de domini i altres paràmetres útils. DNS resolver, el gastarem com a DNS primari. Portal captiu, possibilitat de fer un Hotspot per a la wifi pública. Obliga els usuaris a autenticar-se abans de concedir accés a Internet. Quan siga possible, el tallafoc presenta automàticament una pàgina web d’inici de sessió en la qual l’usuari ha d’introduir credencials com ara un nom d’usuari/contrasenya, un codi de val o un simple acord de clic. També el podem configurar obert, que presente una fulla de presentació de la sala i encaminar després al servidor web amb informació de l’obra. DDNS, per a configuració del DNS dinàmic i la renovació de credencials. "],["requeriments.html", "5.2 Requeriments", " 5.2 Requeriments La distribució de pfSense® és compatible amb la majoria de maquinari compatible amb FreeBSD, són compatibles amb la màquina d’arquitectura de 64 bits (amd64, x86-64). Requisits mínims CPU compatible amb amd64 (x86-64) de 64 bits 1 GB o més de RAM, El mateix sistema operatiu, juntament amb altres serveis, requerirà almenys 175-256 MB de RAM addicional i possiblement més segons les funcions utilitzades. Els tallafocs en entorns que requereixen un gran nombre d’estats simultanis han de tenir suficient RAM per a contenir la taula d’estats. Cada estat necessita aproximadament 1 KB de RAM. Cada connexió a través del tallafoc consumeix dos estats: un entrant al tallafoc i un altre que surt del tallafoc. Unitat de disc de 8 GB o més (SSD, HDD, etc.) Una o més targetes d’interfície de xarxa compatibles, Broadcom 5720, amb driver bge. Problemes específics de la targeta Broadcom bce, fer el que recomana el manual, La del servidor recomanat és bge, no hauria de tindre problemes. Opcional, Suport de l’accelerador criptogràfic. El servidor recomanat te AES-NI pag. 17 del manual. Recomanat per a VPN siga més fluid, i no sature la CPU. No cal seleccionar res perquè OpenVPN utilitze AES-NI, natiu. Unitat USB d’arrencada o unitat òptica d’alta capacitat (DVD o BD) per a la instal·lació inicial "],["installació.html", "5.3 Instal·lació", " 5.3 Instal·lació Mitjan un dispositiu Netgate o viralitzant que és el que farem servir. Descarreguem la imatge AMD64 ISO Estem en fase de proves, farem la instal·lacióó en Virtual Box. Configuració de prova VBox pfSense configuració Proxmox Recomanem fer servir hypervisors de tipus 1 per a ús de producció. Els hypervisors de tipus 2 com ara VirtualBox o VMware Workstation funcionen bé per a les proves, però eviteu fer servir-los per a funcions de producció sempre que siga possible. La forma d’instal·lació és com qualsevol VM, hem baixat la versió 2.6 En l’opció 2 definim les ip de les targetes, pfSense les detecta automàticament, pero les he canviat per les proves. En el meu cas de moment em0 és 192.168.122.59 amb un pont linux a la primera ethernet real del servidor (no exactament, és un pont a l’ethernet de proxmox que comunica a la ethernet del servidor real), per on entra internet, per a la WAN (DHCP de Proxmox per al vmbr0). La em1 interna LAN 192.168.56.10 per comunicar amb vmbr1 Open vswitch (switch intern virtual de les VM), la canviarem més avant, quan configurem las VLAN i el DHCP de pfsense per aquestes xarxes. Es recomana ser una 172.16.x.x si es vol fer VPN. La em2 la que comunicarem amb un pont linux vmbr2 amb la segona targeta real del servidor, que comunicara amb el switch físic de la sala.(l’he connectat a la targeta de xarxa del portàtil per fer les proves cap a fora) Per configurar-lo, millor no donar d’altade moment la LAN (em1), o en el proper reinici, el pfSense tanca els ports WAN, i ja no podem comunicar-nos amb la GUI web per configurar, en VirtualBox no hi ha problema, la LAN la conectes al pont intern amb el host (vbnet0) i pots entrar a aquesta IP (192.168.56.40) per configurar, Pero en Proxmox no és tan evident, hauria d’estar la em2 en vmbr2 configurada, i connectaré al switch real i d’aci a la xarxa LAN, o crear una VM amb un ubuntu per exemple, connectat al vmbr1 i des d’aci configurar el pfSense. El que tenim en consola VirtualBox Virtual Machine - Netgate Device ID: 8258d4ba8668f91ae9d7 *** Welcome to pfSense 2.6.0-RELEASE (amd64) on pfsense *** WAN (wan) -&gt; em0 -&gt; v4: 192.168.122.59/24 LAN (lan) -&gt; em1 -&gt; v4: 192.168.56.10/24 1) Logout (SSH only) 9) pfTop 2) Assign Interfaces 10) Filter Logs 3) Set interface(s) IP address 11) Restart webConfigurator 4) Reset webConfigurator password 12) PHP shell + pfSense tools 5) Reset to factory defaults 13) Update from console 6) Reboot system 14) Disable Secure Shell (sshd) 7) Halt system 15) Restore recent configuration 8) Ping host 16) Restart PHP-FPM 9) Shell Enter an option: Connectant via web, hem d’accedir des de la xarxa LAN, per defecte el tallafoc bloca la GUI des de la WAN. pfsense gui La resta de configuracions es pot fer mitjançant aquesta GUI. "],["configuració.html", "5.4 Configuració", " 5.4 Configuració Comencem la configuració del pfSense, es pot fer pel wizard o manualment, descriurem breument la manual. 5.4.1 Informació general En sistema -&gt; configuració general, donarem nom al host i el domini, així com l’idioma de la GUI, NTP, DNS primari. El nom del host sera pfsense, i el domini inestable.dedyn.io, ja que s’ha tret un conte en el DDNS de deSEC amb aquest nom. pfsense_conf1 El servidor DNS es poden deixar en blanc si el DNS Resolver està actiu mitjançant la configuració predeterminada. La configuració predeterminada té el DNS Resolver actiu en mode de resolució (no en mode de reenviament), quan s’estableix d’aquesta manera, el DNS Resolver no necessita reenviar servidors DNS, ja que es comunicarà directament amb els servidors DNS arrel i altres servidors DNS autoritzats. Per forçar el tallafoc a utilitzar aquests servidors DNS configurats, activeu el mode de reenviament al DNS Resolver. Més tard podem fer un ns2 rèplica i configurar aquesta opció dels dos servidors. 5.4.2 Configuració WAN Aquesta és la xarxa externa que s’enfronta a l’ISP o a l’encaminador amunt, de manera que l’assistent ofereix opcions de configuració per admetre diversos tipus de connexió d’ISP comuns. Tipus WAN, El tipus seleccionat ha de coincidir amb el tipus de WAN requerit per l’ISP, l’opció predeterminada és DHCP a causa del fet que és el més comú, aquesta configuració permet que un tallafoc “Just Work” sense configuració addicional. Configuracio WAN Seleccionem en el tipus DHCP i que ens proporcione una ip l’encaminador, la podríem posar en estàtica i ens apareix un nou menu on li donaríem una ip en la mateixa xarxa que l’encaminador, una màscara i la direcció del gateway. 5.4.2.1 Bloqueja les xarxes Privades RFC 1918, Bloqueja les connexions procedents de xarxes privades registrades d’entrar a la interfície WAN. (10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16) Normalment, aquesta opció només és desitjable en interfícies de tipus WAN per evitar la possibilitat que arribe trànsit numerat de manera privada per una interfície pública. Bogon, el tallafoc bloqueja l’entrada de trànsit si prové d’espai IP reservat o no assignat que no s’hauria d’utilitzar. La llista de xarxes bogon s’actualitza periòdicament en segon pla i no requereix cap manteniment manual. Aquesta opció, Bogon, només hi ha de fer ús en interfícies externes (WAN), no és necessària en interfícies locals i pot bloquejar el trànsit local necessari. Seleccionem els dos bloquejos. Bloqueja les xarxes 5.4.2.2 Passerella La definim per a WAN, no per a les LAN o tracta aquestes com si foren WAN en l’àmbit de tallafoc. Ho definim en Sistema -&gt; routes Li donem la direcció del gateway del encaminador, eixida a internet. Passerella de la WAN 5.4.3 Configuració LAN Tenim dues opcions Si aquest tallafoc no es connecta a cap altra xarxa mitjançant VPN, és possible que la 192.168.1.0/24 xarxes predeterminada siga acceptable. Si aquesta xarxa s’ha de connectar a una altra xarxa, mitjançant VPN des d’ubicacions remotes, triarem un rang d’adreces IP privades molt més fosc que el predeterminat comú de 192.168.1.0/24. L’espai IP dins del 172.16.0.0/12 blocs d’adreces privades RFC 1918 és generalment el que s’utilitza amb menys freqüència, així que trieu alguna cosa 172.16.x.x per 172.31.x.x evitar les dificultats de connectivitat VPN. Si habilitem interfaz ens tancara l’entrada per la xarxa 192.168.122.0, Compte an aço. Si un client remot es troba en un punt d’accés sense fil fent servir 192.168.1.x (molt comú), el client no es podrà comunicar a través de la VPN. En aquest cas, 192.168.1.x és la xarxa local del client al punt d’accés, no la xarxa remota a través de la VPN. Configuració LAN De moment ho deixarem en la 192.168.56.x per acabar la integració de tots els serveis, i ho modificarem en implementar la VPN. Si seleccionem una passarel·la, el tallafoc s’encarregarà de tractar aquesta interfície com una interfície de tipus WAN per a NAT i funcions relacionades. Això no és desitjable per a interfícies internes com ara LAN. Les passarel·les encara es poden utilitzar en aquestes interfícies per a rutes estàtiques i altres finalitats sense seleccionar una passarel·la aquí a la pàgina d’interfícies. 5.4.4 Configuració de les VLAN Creem 4 VLAN sobre la targeta de la LAN em1 per tindre separats els serveis en diferents xarxes. Control, per l’administració VLAN 1 xarxa 172.16.0.0 És per la que tindrem accés ssh a les VM, el tràfic administratiu, com LDAP i la compartició d’emmagatzemament de les VM. Privat, per al personal de l’empresa VLAN 20 xarxes 172.16.20.0 Accés a serveis web de Nextcloud, Zoneminder. Video, per a les video ip VLAN 30 xarxa 172.16.30.0 Xarxa sense internet per a la transmissió de video. Pública, per al públic VLAN 40 xarxa 172.16.40.0 Sol accés a internet i servidor web dedicat per informació. Anem a interfases -&gt; VLAN i afegim una nova. Les farem en la interfaz LAN, la em1. Etiqueta VLAN els numere de la VLAN 1,20,30,40 Descripció li posarem el nom de la xarxa. Guardem Crea les VLAN Després d’afegir-les, ens quedaria Les 4 VLAN Després les hem d’assignar, les assigna a noves Ethernet’s que ha creat internes en pfSense Assignació d’interfaces Igual que hem fet amb la interface LAN, configurem les interfaces de les VLAN que hem creat, donant una xarxa per a cada una. Per exemple, la de privat li donem la xarxa 172.16.20.0/24 amb ip estàtica 254 L’habilitem Donem nom Elegim IP estàtica Li donem una IP i màscara de xarxa Guardem VLAN control IP de les xarxes Xarxa Control 172.16.0.0/24 Xarxa Privat 172.16.20.0/24 Xarxa video 172.16.30.0/24 Xarxa Pública 172.16.40.0/24 Imatge 5.1: (ref:sheets-option-drag) 5.4.4.1 Servidor DHCP per a les VLAN En últim lloc, habilitarem el servidor DHCP per aquesta VLAN En serveis -&gt; DHCP elegim la VLAN que volem configurar. En la VLAN privada li donem l’assignació des de la 50 a la 200. Reservant les primeres 60 per a futurs serveis habilitem el servei Elegim el rang de direccions que volem assignar. Configurar rang IP Es pot configurar els DNS, el Domini, servidor LDAP En servidors DNS li diguem que el primer és el de la nostra xarxa local, en aquest cas 172.16.20.254, donem també un d’OpenDNS 208.67.220.220 i el de Google 8.8.8.8, si no es posa res, te els de la WAN per defecte, es per si volem posar un específic per aquesta xarxa. ::: {.rmdinfo .centre data-latex=“{Pendent}”}es pot habilitar el servidor Bind i configurar servidor DNS per zones, a cada zona li assignem soles les resolucions que necessite. ::: Per a la xarxa pública, no li donarem accés al nostre DNS, soles als de fora. Més avant la idea és donar soles el registre del servidor web d’informació d’obres Baix de tot, és on configurarem les direccions estàtiques per als serveis en cada VLAN Direccions estatiques DHCP estàtica configuració "],["configuració-dns.html", "5.5 Configuració DNS", " 5.5 Configuració DNS Configurem el DNS de pfSense De moment farem una configuració bàsica, pero més avant la idea és instal·lar el connector de Bind i fer-ho més professional. En Serveis -&gt; DNS resolver En la configuració general, per defecte, el manual diu que ho fem per a tots les interfaces de xarxa. No és el nostre cas. Voldríem unes per a la xarxa d’administració. Soles les càmeres i el servei Zoneminder per a la de video. La privada el servei nextcloud, zoneminder i si afegim algun mes. I per a la pública, que no consulte el nostre DNS o si s’afegeix un servidor web, soles a aquest. Pero de moment en les proves posem que ho puguen consultar totes, ja es refinara és avant. Configuracio basica de DNS Després d’afegir alguns registres quedaria d’aquesta forma registres DNS Per configurar un registre li donem a afegir i omplim la configuració que ens demana Nom, soles el nom, sense el Domini La IP Descripcion Baix de tot li podem afegir àlies, per exemple ns1 Afegir registre CNS La configuració és provisional per a les proves, En un futur es posaran les IP de les VLAN on s’ofereix el servei cada màquina. "],["firewall.html", "5.6 Firewall", " 5.6 Firewall Documentacio Fem les ragles de firewall per a les VLAN La VLAN 1 tindrà accés a ella mateixa, ja que totes le màquines que volem administrar tenen una interface en aquesta xarxa. La Vlan 20 Li donarem acces a internat i als ports web 80 i 443 ( al STUN si configurem més avant per a les comunicacion de Talk Nextcloud) La VLAN 30 Soles a ella mateixa, blocant internet. La VLAN 40 Soles a internet i port web d’ella si posem el servidor web promocional. Primer que res, hem de bloquejar l’accés a la web de pfSense per a les VLAN 20 30 40, per fer aço fem un alies en firewall que tinga totes les pasareles de la diferent VLAN inclosa la LAN si és que no està desactivada. Tipus host, seleccionem les IP del pfSense en totes les xarxes. Firewall -&gt; alies Alies porta d’enllaç GUI pfSense També creem un alies per a les xarxes internes RFC 1918 les 192.168.0.0/16, 10.0.0.0/8, 172.16.0.0/12 de la mateixa forma pero ara en comptes de host el tipus ha de ser xarxes. Bloquejar l’accés a xarxes privades només permetria l’accés a Internet. Xarxes internes Quedaria Alies pfSense Anem a regles de firewall flotant i creem una regla per impedir que determinades VLAN 20 30 40 accedisquen a la GUI de pfsense. Accio blocar Interfaz les xarxes que volem blocar: privat, pública i video Direcció entrada Protocol TCP Font Qualsevol Destí, hem d’elegir unico host o alias per poder triar el alias que acaben de definir pfSenseGUIAcces Rango, el rang de ports a blocar, el 443 (hem de tindre configurat el pfSense perquè soles done la GUI en el port https, o elegir el port que estem fent servir, si l’averem canviat) Descripció Bloca GUI pfSense VLAN 20 30 40 5.6.1 VLAN 1 Una VLAN administrativa amb accés a qualsevol persona i qualsevol cosa que vulga. Només cal que creeu una regla on qualsevol cosa d’aquesta xarxa puga accedir a tota la resta. Afegim una regla en la VLAN administració on qualsevol cosa d’aquesta xarxa puga accedir a tota la resta. Li donem pas La interfaz administració Font Administració net Destí qualsevol Regal xarxa administració Quedaria VLAN administrracio regles 5.6.2 VLAN 20 Acces a internet i la mateixa xarxa, on estan els serveis Web Nextcloud, Zoneminder i entre ells per comunicació directa per exemple el xat. Cap amfitrió d’aquesta xarxa NO POT accedir a la xarxa d’administració (redundant però més segur d’aquesta manera). Privat bloca xarxa Administració La xarxa es pot comunicar amb ella mateixa. Privat amb ella mateixa Qualsevol host de la xarxa de convidats POT accedir a la passarel·la Privat accedir al gateway Qualsevol host de la xarxa de convidats pot accedir a qualsevol cosa. Privat accés internet Quedaria Privat bloca xarxa Administració 5.6.3 VLAN 30 soles interna, no internet Els amfitrions d’aquesta xarxa poden interactuar entre ells, però res més. Afegim les següents regles Cap amfitrió d’aquesta xarxa NO POT accedir a la xarxa d’administració Video blocar administració La xarxa es pot comunicar amb ella mateixa. Video pas amb ella mateixa Aquesta xarxa no es pot comunicar amb res. Video blocar internet Quedaria video regles 5.6.4 VLAN 40 Soles internet Els usuaris d’aquesta VLAN poden accedir a Internet i res més. Afegim les següents regles Cap amfitrió de la xarxa de convidats NO POT accedir a la xarxa d’administració Pública bloca xarxa admin Qualsevol host de la xarxa de convidats POT accedir a la passarel·la (això és el que proporciona accés a Internet). Publica pas al gateway Cap amfitrió de la xarxa de convidats NO POT accedir a cap adreça privada. (això bloqueja tot l’accés a qualsevol cosa a la xarxa d’àrea local). El alies que vam crear private networks com a destí. Pública bloca xarxes internes Qualsevol host de la xarxa de convidats pot accedir a qualsevol cosa. (aquesta darrera regla permet l’accés a Internet) Publica accés internet Quedaria Publoca pas al gateway 5.6.5 LAN En principi estaria deshabilitada, pero encara no he arribat a plantejar-me aquest problema, si cau pfSense, connectaríem amb la LAN a Proxmox i des d’aci arreglariem el problema, segurament pel i-DRAC pero no he mirat res d’aço de moment. "],["dns-dinàmic.html", "5.7 DNS dinàmic", " 5.7 DNS dinàmic Configurem un DNS dinàmic per poder connectar en el servidor des de l’exterior per VPN, traguem un conte en un DDNS per exemple en deSEC i amb el password que ens dona i el token configurem el servei. Si ens loguem podem configurar els registres DNS del nostre domini. inestable.dedyn.io deSEC loguin Dins del nostre domini, podem afegir subdominis o noms de màquines de serveis que després farem servir per a donar d’alta els certificats letsencrypt Afegim CNAME de servei nextcloud que apunte a la nostra IP pública. CNAME Nextcloud Registrem el nom dels serveis que ens facen falta, d’exemple tenim el TALK de Nextcloud per si el volem posar per un nom de DNS diferent per habilitar el protocol STUN per millorar les comunicacions de video. Collabora per a Nextcloud, que és la versió web del libreoffice. La podem tindre per Docker en la màquina de Nextcloud o crear una VM per a ella, si la càrrega de treball és alta i no volem afectar el rendiment de Nextcloud. (no crec que siga el cas pel número de treballadors, pero ho deixem preparat) Nextcloud, clar Zoneminder, el servei de video. La resta de serveis són interns a la xara VLAN 1 d’administració, i en un certificat auto firmat aquestasta be. Tampoc volem donar pistes dels serveis que tenim. (sense contar en esta guia, clar) CNAME domini Fent un ping al domini, ens porta enkidu@enkidu:~$ ping inestable.dedyn.io PING inestable.dedyn.io (193.111.55.165) 56(84) bytes of data. 64 bytes from 193.111.55.165 (193.111.55.165): icmp_seq=1 ttl=62 time=10.5 ms 64 bytes from 193.111.55.165 (193.111.55.165): icmp_seq=2 ttl=62 time=53.8 ms enkidu@enkidu:~$ ping nextcloud.inestable.dedyn.io PING inestable.dedyn.io (193.111.55.165) 56(84) bytes of data. 64 bytes from 193.111.55.165 (193.111.55.165): icmp_seq=1 ttl=62 time=6.77 ms enkidu@enkidu:~$ dig inestable.dedyn.io ; &lt;&lt;&gt;&gt; DiG 9.16.15-Ubuntu &lt;&lt;&gt;&gt; inestable.dedyn.io ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 23673 ;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 65494 ;; QUESTION SECTION: ;inestable.dedyn.io. IN A ;; ANSWER SECTION: inestable.dedyn.io. 60 IN A 193.111.55.165 ;; Query time: 135 msec ;; SERVER: 127.0.0.53#53(127.0.0.53) ;; WHEN: dl. de juny 06 18:52:32 CEST 2022 ;; MSG SIZE rcvd: 63 enkidu@enkidu:~$ dig @8.8.8.8 nextcloud.inestable.dedyn.io ; &lt;&lt;&gt;&gt; DiG 9.16.15-Ubuntu &lt;&lt;&gt;&gt; @8.8.8.8 nextcloud.inestable.dedyn.io ; (1 server found) ;; global options: +cmd ;; Got answer: ;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 49066 ;; flags: qr rd ra ad; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1 ;; OPT PSEUDOSECTION: ; EDNS: version: 0, flags:; udp: 512 ;; QUESTION SECTION: ;nextcloud.inestable.dedyn.io. IN A ;; ANSWER SECTION: nextcloud.inestable.dedyn.io. 3600 IN CNAME inestable.dedyn.io. inestable.dedyn.io. 60 IN A 193.111.55.165 ;; Query time: 179 msec ;; SERVER: 8.8.8.8#53(8.8.8.8) ;; WHEN: dl. de juny 06 19:20:19 CEST 2022 ;; MSG SIZE rcvd: 87 La IP 193.111.55.165 se la publica de casa, no puc provar fent un port forwarding apuntant a la màquina nestcloud, perquè és una connexio per antena, router apunta a una 10.1.X.X, no tinc accés a configurar la IP pública. 5.7.1 Renovacions del servei PfSense s’encarregara de la renovació periòdica del servei, en sevicies -&gt; DNS dinàmic, afegim uno nou en ADD i omplim la plantilla que ens mostra a continuació. En tipo de servei, el DDNS que ens dona el servei deSEC La interfaz, la WAN, que és la que està en contacte en la xarxa rel router. Nom de la host, el nom del nostre domini inestable.dedyn.io Contrasenya li posem el token que ens ha proporcionat el DDNS. Pfsense configuració DDNS "],["vpn.html", "5.8 vpn", " 5.8 vpn openVPN config pfSense Les VPN d’accés remot permeten als usuaris connectar-se de manera segura a una xarxa des de qualsevol lloc on hi haja una connexió a Internet disponible. Per oferir als empleats la possibilitat de treballar des de casa. Amb la proliferació de telèfons intel·ligents, els usuaris tenen la necessitat d’accedir de manera segura als serveis interns des dels seus telèfons mitjançant una VPN d’accés remot. Les VPN d’accés remot es poden configurar de manera que passe tot el trànsit del sistema client a través de la VPN, té un impacte en el rendiment, la farem soles per accedir als serveis de l’empresa. 5.8.1 Autenticació Possibilitats IPsec OpenVPN WireGuard (No recomanable) L’ús d’OpenVPN amb certificats, autenticació TLS i autenticació d’usuari és el mètode més segur i més senzill d’implementar. El xifratge es veu compromés si es comprometen les claus privades de l’estructura PKI, tot i que l’ús de múltiples factors com l’autenticació TLS a la part superior de la PKI pot mitigar part del perill. OpenVPN requereix l’ús de certificats per a l’accés remot a la majoria d’entorns, que inclou la seua pròpia corba d’aprenentatge i pot ser una mica difícil de gestionar. Hi ha un assistent per gestionar les configuracions d’accés remot d’OpenVPN més habituals i els paquets d’exportació de clients d’OpenVPN faciliten el procés de posar en funcionament els clients. OpenVPN té clients disponibles per a Windows, Mac OS X, tots els BSD, Linux, Solaris i Windows Mobile, Android, iOS, però el client no ve preinstal·lat en cap d’aquests sistemes operatius. En funció del desplegament. Els clients mòbils (Accés remot), poden rebre una configuració automàtica en determinats casos. Poden rebre automàticament una adreça IP (recomanat 172…) assignada des d’un grup, i es poden enviar nombroses opcions addicionals per controlar el seu comportament des del costat del servidor (encaminament, DNS i molts altres). 5.8.2 Firewall OpenVPN és molt compatible amb el tallafoc. Com que utilitza un únic port UDP o TCP i no es veu afectat per les funcions NAT habituals. OpenVPN admet NAT entrant (per exemple, reenviaments de ports) i sortint mitjançant la pestanya OpenVPN del grup i també a les interfícies assignades. L’única dificultat possible és si el protocol i el port en ús estan bloquejats. Quan s’assignen com a interfície, les instàncies d’OpenVPN admeten completament les regles per túnel. 5.8.2.1 Regles L’assistent VPN d’accés remot d’OpenVPN ofereix, crear regles per passar el trànsit i el trànsit WAN a la interfície d’OpenVPN. El trànsit encapsulat dins d’una connexió OpenVPN activa es controla mitjançant regles definides per l’usuari a la pestanya OpenVPN a Firewall &gt; Regles. 5.8.3 Configuració òptima de xifratge Feu servir maquinari compatible amb AES-NI. A la configuració de la Fase 1 (IKE), utilitzeu: AES128-GCM amb longitud de clau de 128 bits per a l’algorisme AES-XCBC per al hash, que en aquest cas és efectivament una funció pseudoaleatòria (PRF). Això donarà el màxim rendiment en combinació amb AES128-GCM en maquinari que pot accelerar tots dos (per exemple, AES-NI) A la configuració de la Fase 2 (SA infantil), utilitzeu: AES128-GCM amb longitud de clau de 128 bits per a l’algorisme No seleccioneu cap algoritme hash. Un algorisme hash no és necessari per a AES-GCM, ja que ja inclou l’autenticació. 5.8.4 UDP UDP té menys despeses generals per a les dades tunelades i, si un client ha de retransmetre, no augmentarà el problema retransmetent tant dins com fora del túnel. 5.8.5 Utilitzeu TLS només per a l’autenticació OpenVPN pot utilitzar TLS tant per a l’autenticació com per al xifratge del canal de control. Realitzar el xifratge del canal de control afegeix més sobrecàrrega, que pot augmentar amb molts clients. Si no cal el xifratge del canal de control, considereu fer ús TLS només per a l’autenticació. Independentment de quina opció escolliu, el trànsit transportat per OpenVPN està xifrat. 5.8.6 Utilitzeu la negociació de xifratge de dades La negociació de xifratge de dades es pot usar per a establir preferències de manera que els clients puguen preferir xifrats més eficients quan siga possible, però es poden fer úsr altres quan siga necessari. Estableix primer seleccions d’alta prioritat com ara AES-128-GCM, seguides d’altres com AES-128-CBC. 5.8.7 Túnels dividits El túnel dividit només envia trànsit per a subxarxes específiques a través de la VPN en lloc d’enviar tot el trànsit. Amb OpenVPN, això es pot fer desmarcant les opcions de redirecció de la passarel·la IPv4/IPv6 i configurant les entrades de xarxes locals IPv4/IPv6. Tanmateix, els clients encara poden anul·lar aquest comportament de forma remota, així que comproveu també les configuracions del client. 5.8.8 Desactiva la compressió La majoria de les dades trameses a través de VPN en entorns moderns ja estan xifrades o no es poden comprimir, cosa que malgasta la CPU quan s’intenta comprimir. A més, vulnerabilitats com VORACLE poden permetre als atacants obtenir informació sobre dades xifrades quan s’han comprimit. 5.8.9 Connexions duplicades Normalment, si un client OpenVPN es connecta utilitzant el mateix nom d’usuari o certificat CN, la connexió anterior es trenca a favor de la nova connexió. Això és més segur, però no permet que cap usuari es connecte diverses vegades. És el cas que tenim, pot estar connectat via web i via app movil. 5.8.10 Augmenta la memòria intermèdia d’enviament/recepció La mida de memòria intermèdia predeterminada és segura, però no òptima. Augmentar la mida de la memòria intermèdia fins a 512 KiB a ambdós costats pot augmentar el rendiment. 5.8.11 OpenVPN i certificats La millor pràctica és emprar certificats per a l’accés remot i les VPN de lloc a lloc perquè permet revocar l’accés per a clients o llocs individuals. Idealment, els certificats haurien de ser únics per dispositiu o almenys per usuari. Si un client amb un certificat individual està compromés o l’accés s’ha de revocar per qualsevol altre motiu, simplement revoqueu aquest certificat. No hi ha cap altre client afectat. La GUI del programari pfSense inclou una interfície de gestió de certificats totalment integrada amb OpenVPN. Les autoritats de certificació (CA) i els certificats de servidor es gestionen al Gestor de certificats a la interfície web, que es troba a Sistema &gt; Gestor de certificats. Els certificats d’usuari també es gestionen a la interfície web, com a part del gestor d’usuaris integrat que es troba a Sistema &gt; Gestor d’usuaris. Els certificats es poden generar per a qualsevol compte d’usuari creat localment al tallafoc, excepte el compte d’administrador predeterminat. Peer to Peer (SSL/TLS) Peer to Peer (clau compartida) Accés remot (SSL/TLS) Accés remot (autenticació d’usuari) Accés remot (SSL/TLS + autenticació d’usuari) Utilitzarem Accés remot (SSL/TLS + autenticació d’usuari) Aquesta és l’opció més segura disponible. No només obté els avantatges d’altres opcions SSL/TLS, sinó que també requereix un nom d’usuari i una contrasenya del client quan es connecta. L’accés del client es pot eliminar no només revocant el certificat, sinó també canviant la contrasenya. A més, si no es descobreix immediatament una clau compromesa, el perill es redueix perquè és poc probable que l’atacant tinga les claus i la contrasenya. 5.8.12 Mode de dispositiu OpenVPN es pot executar en un dels dos modes de dispositiu: tun: Funciona a la capa 3 OSI i realitza l’encaminament en interfícies punt a punt. Toc: Pot funcionar a la capa 2 OSI i pot dur a terme tant l’encaminament com el pont si és necessari. Els clients com els que es troben a Android i iOS només admeten el mode tun a les aplicacions que la majoria de la gent pot utilitzar. Farem servir aquest mode. 5.8.13 Configuració oepenvpn pfsense Configure the OpenVPN Server in pfSense with the Best Security Primer que res instal·larem el paquet que genera automaticament les configuracions per als clients. En sistema grent d’empaquetacio instal·lem openvpn-client-export client export OpenVPN Després hem de crear els certificats digitals en el propi pfSense, s’ha de crear una autoritat certificadora CA que firmara els certificats. Hem de crear un certificat per al servidor OpenVPN de tipus servidor, i els certificats VPN dels clients. En sistema -&gt; gerent de certificats des d’aci tambe podem revocar certificats si un usuari el perd. 5.8.13.1 CA interna Creem la CA li donem a ADD Nom descriptiu OpneVPN_usuaris Elaboracio CA interna Tipus de clau ECDSA Algoritme resum sha512 vida util 3650 10 anys Nom cumu, un nom per a identificarlo openvpn-usuaris-CA Quedaria CA per a OpenVPN Apareixeria en ñe CAs del sistema CAs del sistema Des d’aci, podem veure si està en ús i quants certificats té actius. 5.8.13.2 Certificat per al servidor de VPN Ara crearen el certificat per al servidor OpenVPN intengrat en pfSense. Anem a la seccio certificats i li donem a ADD molt paregut a l’anterior. Elaboracio certificat interior Autoritat, el certificat creat per a OpenVPN d’abans Clau ECDSA Algoritme sha512 Vida util 3650 10 anys Nom cumu, un nom per a identificarlo openVPN-servidor Certificat per al servidor OpenVPN En atributs del certificat, li hem de dir que és per a servidor, el reste ho podem deixar en blanc. Atributs certificats servidor VPN Aquest és el certificat que li hem de donar en configurar el servidor VPN Certificats dels clients El mateix proces que hem fet amb el servidor pero ara elegirem en comptes de servidor , client. Certificat per als clients Ara podem veure que apareix en la llisra dels certificats. Llista de certificats Si els editem, els podem exportar amb un password que cifraria el certificat amb ell. editar certificats 5.8.14 Configuracio del servidor En el menu de pfSense anem a la pestany VPN -&gt; OpneVPN -&gt; servodoes -&gt; ADD Hi ha un wizards (asistent) per poder-lo configurar, pero ho farem pas a pas. Eligirem les opcions Descriptcio, podem tindre varis, i de fet farem un altre per administracio externa Mode servidor Acceso remot (SSL /TLS + Aut. de usuario) backend LDAP Turnkey (és el servidor LDAP que tenim instal·lat, tambe es va provar zentyal que és un servidor Active Directori i en ell se’n va instal·lar un FreeRadius) Protocol UDP Mode tipe Tun, que és la que admeten dispositius Android Interface WAN és per la que entrara Local port 1194 per defecte, es recomana que el canviem. Configuracio del servidor OpenVpn primera part Configuerm la part criptografica Habilitem la clau TLS per poder fer uns de tls-crypt i generar automaticament la clau Autoritat de certificacio, la CA que hem creat per a aquest servidor OpenVPN_usuaris peer certificate revocation, si hem creat una llista d’usuaris que no tinguen acces, l’eligiriem aci, es crea en la seccio system server certificate, el certificat de servidor per aquest servei OpneVPNServidor DH parametr length ECDH only Curva ECDH secp521r1 Data Encryption Algorithms Data AES-256-GCM, AES-128-GCM i CHACHA20-POLY1305 Algoritme Fallback per si el client no és compatible, el que ve per defecte AES-256-CBC Auth Digest Algorithm SHA256 Certificado de Profundidad No admet certificats fets per CA intermedi. Configuracio del servidor OpenVpn crypto Configurem la part del tunell Xarxa per als clients, ha de ser una xarxa lliure, que encara no hem utilitzat per defecte 10.8.0.0/24 Pasarela IPv4 de Redirección, si habilitem aquesta opcio, el client tindra acces a totes les xarxes internes, ho farem per al sevidor OpneVPN d’administracio. En el dels clients li direm baix a la xarxa que te acces, la privat VLAN 20, o definir després regle firewall per aquesta connexio i llimitar soles a la VLAN 20 172.16.20.0/24. IPv4 red (s) local la VLAN 20 172.16.20.0/24 Conexiones concurrentes el número de conexions simultanies 15 Compresio, no, és un risc de seguretat, explicat mes a dalt. compresio, no Inter-client communication, comunicacio entre clients, si Duplicate Connection, és perquè client es puga connectar en diversos dispositius amb un sol compte, portatil, movil . Es recomana que no ho posem i creem un certificat per a cada dispositiu, de moment l’activem Configuracio del servidor OpenVpn tunel Configurem la part del client IP dinamica Topologia subred Configuracio ping, és per veure si un client continua conectat, ho deixem com esta. Habilitem el nom de domini predeterminat Li donem el DNS local de la xarxa, el mateix pfSense, i el de Google 8.8.8.8 Configuracio del servidor OpenVpn client Configuracion avanzada Tipus de pasarela IPv4 Configuracio del servidor OpenVpn avançada En guardar per primera vegada es genera la clau TLS en configuracio de cifrat, ara tindriem que tornar i baix canviar el modo de uso de clave a cifrado y autentificacion. canviar mode en cifrat I ja el tindriem configurat. Servidor OpenVPN resum Soles quedaria configurar el firewall de connecxions entrants per deixar que accedeixquen per la WAN En firewall -&gt; WAN afegim una nova regla que deixe pasar interfaz WAN Familia IPv4 protocol UDP Font cualquiera Destination WAN en el port que tenim servitn aquest servidor OpenVpn 1194 regal firewall WAN per a OpenVpn Resum regles WAN Quedaria definir les regles de firewall de la nova interface openVpn que acaba de crear-se Com en el cas de les VLAN anem a interfase -&gt; assignar interfases i l’afegim Afegir interfase openVpn Li donem nom i l’habilitem com sempre habilitar interfase opneVPN I li donem les regles de firewall Acces a la xarxa VLAN 20 privat acces a la VLAN 20 Bloquetjar la d’administracio Blocar a la d’administracio Permet entre elles Permet entre la xarxa vpn De la xarxa vpn a la passarella VPN a la passarella Quedaria Firewall VPN resum Faltaria provar si funciona bé, en casa no puc fer-ho per la meua connexio a internet. No s’ha posat la regla de la net a qualsevol cosa, crec que en aço no li donem acces a internet per la VPN, pero no estic segur. Ara tindriem que fer el mateix per un altre servidor OpenVpn per a les tasques d’administracio amb acces a totes les xarxes 5.8.15 Servidors d’autentificacio Aprofitarem aquesta secio per a definir els servidors d’autenticacio. Hauria de tindre un capitol per ell mateix, pero de moment no he arribat a plantejar-me l’organigrama de l’organitzacio. És un problema que abordare quan ja estiga tot configurat i funcionant. De moment he creat un usuari per probar que funciona. La idea és tindre com a minim 4 o 5 grups Administracio (administrador i colaborador administratiu per donar d’alta i baixes usuàries solament) Nextcloud (tots els usuaris) video (sol usuariper accedir a Zoneminder) VPN, en principi tots els de Nextcloud tindran acces a VPN. Convidats temporals. Ara tenim dues opcions, utilitzar Zentyal que és una versio lliure 100% Active Directori i instalar un servidor Radius, el porta ja com a plugin, perquè comprove els usuaris, o utilitzar OpenLDAP, he utilitzat la versio que hi ha en Turnkey per agilitzar les proves. 5.8.15.1 Configuracio OpenLDAP Instal·lariem la VM com la resta, i la poseriem en la VLAN 1 sol. En la demostracio està en la LAN, simplement seia posar-la en la VLAN 1 després. No afecta la prova. OpenLDAP onfiguracio de xarxa Creem l’usuari de prova per PhpLDAPAdmin Farem una configuracio simple d’un usuari per phpLDAPadmin, en el grup usuari, ja en proces de produccio farem els grups, és soles per provar que ens autentica pfSense Usiari des de PHPLDAPAdmin El seguent pas seria anar a pfSense i en sistema -&gt; gestio d’usuaris -servidors d’autenticacio afegir el servidor LDAP i perquè el volem fer servir, en el nostre cas, per autenticar els usuaris d’OpenVPN que acabem de crear. Nom, LDAP turnkey Tipus LDAP IP, li posariem el nom DNS LDAP.inestable.dedyn.io o la IP El port si l’hem canviat, per defecte el 389 Transport, s’hauria d’usar SSL/tsl, pero tindriem que crear el certificat per al servidor … Ho deixem per mes avant, posem TCP standart, que no va encriptada, com anira per una VLAN sola d’administracio, no seria un problema crític, pero millor encriptar. Peer autoritat certificacio, aci li posarem perquè el volem, per autenticar els usuaris d’OpenVPN OpenVPN_usuaris que el que vam crear. Base DN dc=inestable,dc=dedyn,dc=io Contenedors d’autenticació ou=Users,dc=inestable,dc=dedyn,dc=io Enllaç anònim com el tenim habilitat, ho marquen i no hem de posar l’admin ni el seu password perquè realitce les cerques, no és recomanable. Atribut de nom d’usuari he posat uid, pero podria ser CN La resta és per refinar més la recerca LDAP, es mirara ms avant. Pendent Configuracio autenticacio OpenLDAP Comprovarem que funciona, anem a diagnòstic -&gt; autenticacio i en el servidor que acabem de configurar posem el uid del usuari i el password per veure que funciona. Comprovant OpenLDAP 5.8.15.2 Configuracio Zentyal Es comporta com un Active Directori, la veritat és que en la versio gratuita, la GUI té apartats llimitats, en principi ens és indiferent configurar la xarxa com una AD Windows, soles la volem per autenticar usuaris per als serveis, la resta com samba, correu … No els implementarem. L’entorn de configuracio és més amigable que el PHPLDAPAdmin, és una opcio a tindre en compte, si deixem l’administracio d’usuaris a una tercera persona, Tambe utilitza més recursos de la màquina server. Creem un usuari en zentyal. En users and computers -&gt; users li donem al + Setup Radius For Authentication With A pfSense VPN Server Farem el mateix, configurarem un usuari basic per provar que l’autentica. Afegir usuari a Zentyal Comprovem que està afegit Comprovem l’usuari a Zentyal En gestio de Software l’hem d’installar, el paquet RADIUS, des d’aci és on podem instal·lar més serveis con DHCP, FTP, correu … i ho configurariem des del GUI, Es el que fa interesant aquesta opcio. Una vegada ja instal·lat anem a RADIUS i el configurem elegiriem a quin grup es que volem que s’autentique, en el nostre cas soles tenim un grup usuaris, o podem elegir a tot el món. (seria posar en un futur un grup VPN) Configurem el client RADIUS li donem un nom, en el nostre cas volem que siga pfSense La IP del client 192.168.56.254 en un futur, seria la de la xarxa VLAN 1 172.16.0.254 I definim una clau compartida que l’hem d’introduir després en el pfSense. Configurar RADIUS server Zentyal Pasariem a pfSense Afegiriem un altre servidor d’autenticacio Nom Radius Tipus Radio Protocol ms-chapv2 Nom o ip, el del servidor Zentyal 192.168.56.14, en un futur la de VLAN 1 secret compartit, la clau que hem definit abans en el servidor RADIUS L’últim és per on escoltara les peticions, per la WAN que és per on entra OpenVN, pero podrien definir-loper a una altra xarxa per si volem utilitzar usuari i password per exemple per conectar per wifi a les VLAN mode enterprise, per a aquest fi, pfsense porta tambe per instal·lar el paquet freeRadio on es pot configurar un altre servidor Radio que conecte en LDAP i valide els accesos a les VLAN…. Configureu el servidor FreeRADIUS a pfSense i utilitzeu WPA2 / WPA3 Enterprise Configuracio client RADIO pfSense Comprovem que podem autènticar amb aques metode Comprovacio autenticacio RADI Resum metodes d’autenticacio, es podria posar tambe per a Zential tipus LDAP Resum dels metodes d’autenticacio 5.8.16 Exportar els arxius de configuracio VPN dels clients Si vam instal.lar el mòdul OpenVPN-export anem a VPM -&gt; OpenVPN -&gt; Client Export Per Exportar directament en un paquet instal·lable la configuracio per als clients, aquest sol han d’executar-lo i queda tot configurat. Hi ha per a Windows, Mac, Android, IOS… El baixem i l’enviem al client. Remote acces servidor, el que tenim configurat, el dels usuaris Resolució del nom d’amfitrió, El domini que tenim en el DDNS inestable.dedyn.io Verifiqueu el CN del servidor CN: Automatic – use verify-x509-name Block outside DNS habilitada Mode d’enllaç port d’origen aleatori Guardem Exportem per al tipus de dispositu que volem Exportacio certificat client Exportacio certificat paquet Els podem encriptar per a distribuir-los password per al certificat Soles apareix OpenVPNSclient que es el que hem creat, soles un, pero podriem, i deuriem crear per usuaris individuals, per si els perden poder-los revocar, sence afectar els altres o per a treballadors i convidats. Baixem el d’android Paquet VPN per Android Soles quedaria provar a vore si funciona. 5.8.17 Configuracio VLAN en ubuntu host En l’iface de vboxinterna de pont sudo apt install vlan Si el modul no esta actiu , el carreguem sudo modprobe --first-time 8021q Per provar si funciona sudo ip link add link vboxnet0 name privat.400 type vlan id 400 sudo ip link add link vboxnet0 name public.200 type vlan id 200 sudo ip link add link vboxnet0 name video.300 type vlan id 300 Que és la VLAN privat que hem definit Li donem una ip sudo ip addr add 172.16.40.10/24 dev vboxnet0.400 Fem ip per veure que anat be. O podem fer sudo dhclient -v Per obtenir les ip del dhcp de cada VLAN "],["truenas.html", "Capitol 6 TrueNas", " Capitol 6 TrueNas Truenas és una solucio NAS lliure basada en FreeBSD. Farem servir aquesta plataforma per a compartir espais en la resta de VM, i tindre d’una forma centralitzada tots els fitxers per poder fer còpies de seguretat més eficients i facilitar la gestio. "],["installar-truenass.html", "6.1 Instal·lar Truenass", " 6.1 Instal·lar Truenass Instal·lem TrueNass, creant un amfitrió de virtualització Proxmox amb un servidor NAS integrat. Instal·leu el NAS com qualsevol altra màquina virtual amb el seu sistema operatiu residint en un disc virtual en vmZFS. Les unitats d’emmagatzematge reals residiran a l’emmagatzematge físic zfspool1 assignat mitjançant una tècnica coneguda com a pas de maquinari. Creem una nova màquina virtual com sempre amb la següent configuració. Configuracio de la VM TrueNas El disc sata1 que es veu en la configuració, l’afegirem ara seguint els següents passos. Arranquem i comença la instal·lacio. Instala.lació de Truenas De moment soles tenim el disc dur per instal·lar el sistema base, 32 Gib que està en l’espai vmZFS del host. L’altre, el d’espai, l’afegirem després. Elegir disc per la instal·lació Seguim els passos fins al final, com qualsevol SO, i reiniciem. Pimera arrancada Des d’aci podríem configurar la xarxa i canviar al password del root per poder entrar des del navegador. La xarxa de moment no la toquem fins que no estiga tot configurat en pfSense, i el passem a les VLAN que li adjudiquem. Truenas GUI L’usuari que pot entrar és root amb el password que li hem posat en el pas anterior. Truenas GUI web 6.1.1 Afegirem disc de pas, discs reals D’aquesta forma tenim separades en dos discs, la VM i les dades. Aquest mètode permet una còpia de seguretat del sistema molt petita, perquè només feu una còpia de seguretat del sistema operatiu VM Truenas i NO de les dades. També proporciona un rendiment de disc moderadament millor per als cicles de lectura/escriptura El més gran avantatge és que les dades NO s’emmagatzemen en un fitxer de disc virtual. Si es destrueix la VM, les dades estan segures en un disc real, que es pot muntar amb qualsevol altre SO. 6.1.2 Afegim el disc a Truenas Per buscar els discs durs en Trueas cat /proc/partitions Podem crea un LVM o un ZFS, per aquest espai Afegim 3 discs durs, tenim dues opcions Fer una RAID0 en dos discs i l’altre disc per fer les còpies de seguretat de les parts que ens interesse guardar. En el primer volum, de dos discs, és on guardarem les dades del nextcloud, els fitxers de la part administrativa, les captures de video i tots els espais de disc compartits amb els serveis que oferirem ara o en un futur. En el segon volum, el tercer disc dur, farem una rèplica de les dades de Nextcloud i serveis, la part administrativa i la carpeta de videos editats. ::: {.rmdnote data-latex=“{Nota}”} Per la forma de treballar de l’empresa, no ens interessa còpies de seguretat de totes les captures de video, ja que es vol oferir com un servei per a les companyies externes que actuen, i després esborrar-les. Soles les còpies de les obres pròpies es voldrien guardar. Si fem un RAID 1 carreguem el sistema TrueNas en càlculs, fent el disc de paritat, quan la major part de les dades no ens interessa. És molt més rapid, canviar un disc que es trenca i restaurar la còpia de seguretat, que restaurar la paritat d’un sistema RAID 1, on mes de la meitat de les dades que es restauren ens són indiferent. ::: En cas de catàstrofe En cas de catàstrofe d’un dels discs del volum 1, es canvia i es restaura les còpies de seguretat. En cas de desastre en disc de còpies de seguretat, es canvia i es regeneren les imatges de les VM i es torna a fer còpia de seguretat de les dades. En cas de catàstrofe en el ssd on està el proxmox, el canviem, instal·lem altra vegada Proxmos, connectem el volum de còpies de seguretat i restaurem la configuració de proxmox i les VM. Un altre avantatge d’aquest mètode, és que amb RAID0, la velocitat del disc format d’aquesta forma és més ràpit. Ja que reparteix la lectura i escriptura entre els dos discs. Si li donem molta resolució a les càmeres, igual si vindria bé aquest plus de velocitat enfront del de seguretat, si al mateix temps estan utilitzant el Nextcloud per llegir fitxers. Fer un RAID 1 en els tres discs. Si falla un, el canviem i es regenera la paritat. 3 SATA RAID 1 en els tres discs. Fer un volum en els tres discs i guardar alli el backup, les dades i els videos, cada u en el seu Zvol. En cas de catàstrofe En cas de catàstrofe del ssd, igual que en cas anterior En cas de catàstrofe d’un dels 3 discs, es canvia i es regenera el RAID automàticament, és procés pot ser més lent que el de tornar a copiar les dades de des del backup. Ja que tindríem moltes dades que realment no volem que es tornarien a regenerar, 6.1.3 Afegim els discs a Proxmox Ho he fet de dues formes diferents. La segona em pareix la més correcta, pero deixe la documentació de la primera, perquè part dels següents passos per compartir espai en Nextcloud ho vaig fer en el volum creat d’aquesta forma. No està de més tindre la documentació de com es va fer, per si en algun cas específic, convé més fer-ho d’aquesta manera. En la realitat, li posaríem els discs en les valdes del servidor. En l’edició de proves, hem creat tres discs durs virtuals en format qcow2 en la partició HD del portàtil, no SSD. seleccionar disc durs afegir discs Hem elegit tipus de bus VirtiO que és el que en principi dona més rendiment i ens deixa fer més coses, si dona problemes es pot elegir SATA que és el real. 6.1.4 Volums Ara hem d’elegir el tipus de volum que volem crear, LVM o ZFS. En principi LVM és més pràctic, i els requisits de hardware, en concret de RAM és minim. Arranquem qualsevol linux, muntem el volum, i accedim a les dades. En ZFS el mateix, pero no és tan usual, i els requisits de RAM per aquest volum és més alt. Es recomana minim 4GiB, pero l’òptim son 8GiB, i 1GiB per cada Tera de mes. En el nostre cas, si es compra el servidor en 64GiB de RAM, no seria problema, la resta de VM no tenen gran demanda de RAM. ZFS depén molt de la memòria, de manera que necessiteu almenys 8 GB per començar. A la pràctica, utilitzeu tot el que pugueu obtenir pel vostre maquinari/pressupost. Si no tenim tanta RAM, es pot gastar un disc SSD, sol ser més barat, es pot comprar en altre SSD de menor capacitat per fer aço, no recomane fer-ho en una partició de l’arrel de Proxmox, aço el desgasta molt i tenim mes probabilitat que falle mes ràpidament. Es posa un secundari, i quan falle es canvia, pero si tenim RAM millor, i es el cas. Crea un grup nou amb memòria cau (L2ARC) És possible emprar una partició d’unitat de memòria cau dedicada per augmentar el rendiment (fa ús SSD). El crearíem en cas de falta de recursos. zpool create -f -o ashift=12 &lt;pool&gt; &lt;device&gt; cache &lt;cache_device&gt; 6.1.5 ZFS Elegim ZFS pel format de tots els nostres discs. ZFS és un sistema de fitxers combinat i un gestor de volums lògics dissenyat per Sun Microsystems. L’emmagatzematge ZFS és una combinació de sistema de fitxers i LVM, que proporciona emmagatzematge d’alta capacitat amb funcions importants, com ara protecció de dades, compressió de dades, autocuració i instantànies. ZFS té un RAID definit per programari integrat, que fa que l’ús de RAID basat en maquinari siga innecessari. Una matriu de discs amb ZFS RAID es pot migrar a un node completament diferent i després importar-la completament sense reconstruir tota la matriu. Només podem emmagatzemar imatges de disc virtual en format .raw a l’emmagatzematge ZFS. Mitjançant l’ús de ZFS, és possible aconseguir les màximes funcions empresarials amb maquinari de baix pressupost, però també sistemes d’alt rendiment aprofitant la memòria cau SSD o fins i tot configuracions només SSD. ZFS pot substituir les targetes RAID de maquinari de costos intensos per una càrrega moderada de CPU i memòria combinada amb una gestió fàcil. Avantatges Fiable Protecció contra la corrupció de dades Compressió de dades en l’àmbit de sistema de fitxers Imatges instantànies Clon de còpia sobre escriptura Diversos nivells de raid: RAID0, RAID1, RAID10, RAIDZ-1, RAIDZ-2 i RAIDZ-3 Pot utilitzar SSD per a la memòria cau Autocuració Comprovació contínua d’integritat Dissenyat per a altes capacitats d’emmagatzematge Replicació asíncrona a la xarxa Encriptació Després d’afegir els discs, en proxmox apareixen directament. Tenim els nous discs vdc. vdd i vde Nous discs Per fer RAID0 ho hem de fer des de consola, aquesta opció no la proporciona el GUI, i a més no la recomana, si es perd un disc, es perden les dades. No es pot Crear RAID0 des del GUI Per fer-ho mirem la documentació oficial. RAID0 També s’anomena “striping”. La capacitat d’aquest volum és la suma de les capacitats de tots els discs. Però RAID0 no afegeix cap redundància, per la qual cosa la fallada d’una sola unitat fa que el volum no es puga utilitzar. Quan s’escriu una matriu de paritat com ara RAID (1, 5, 6 o 10), s’ha de sincronitzar la memòria cau de dades sense interrupcions o es degradarà. Si es degrada, es reconstruirà! Si es reconstrueix, introdueix un cicle de treball estés i una acumulació de calor. Aquesta acumulació de calor degrada els components de manera irreversible que condueix a una eventual fallada. Una matriu de striping com ara RAID-0 mai no se sincronitza ni es degrada. Això redueix el cicle de treball i l’acumulació de calor augmentant així la vida útil dels components. Construïm el nostre sistema utilitzant només matrius de striping sense paritat i després fem servir RSync o replica per a replicar les dades en diverses màquines o volums del NAS. Això ofereix el rendiment de RAID-0 mantenint la redundància de dades. Sembla que es compleixen tots els requisits, excepte les sol·licituds de restauració de l’usuari final i minimitzar la corrupció de dades. Per aço tindrem TruNAS amb ZFS. Per crear l’agrupació ZFS zpool create &lt;nom_agrupació&gt; &lt;tipus_raid&gt; &lt;nom_dev1&gt; &lt;nom_dev2&gt; Tipus de RAID ZFS Tipus RAID Parametre RAID0 no string (res) RAID 1 mirror RAIDZ-1 raidz1 RAIDZ-2 raidz2 En el nostre cas li direm zfspool1 i del tipus RAID0 que és no string, és a dir, no posar res. zpool create zfspool1 /dev/vdc /dev/vdd Son /dev/vdx per què son discs virtuals, en el server seran /dev/sdx Per verificar que el grup s’ha creat root@proxmox:~# zpool list NAME SIZE ALLOC FREE CKPOINT EXPANDSZ FRAG CAP DEDUP HEALTH ALTROOT rpool 39G 1.24G 37.8G - - 1% 3% 1.00x ONLINE - vmZFS 99.5G 16.6G 82.9G - - 3% 16% 1.00x ONLINE - zfspool1 99G 105K 99.0G - - 0% 0% 1.00x ONLINE - S’ha creat zfspool1 en l’espai sencer dels dos discs durs Podem utilitzar l’agrupació directament o podem crear un conjunt de dades (dataset) dins de l’agrupació i connectar-lo per separat a Proxmox com a emmagatzematge individual. L’avantatge d’això és aïllar els diferents tipus de dades emmagatzemades en cada conjunt de dades. Aquesta part la farem en Truenas, crearem conjunts de dades per a cada servei. Cada conjunt de dades ZFS es pot configurar individualment amb el seu propi conjunt d’opcions de configuració encriptació, compressió… Podem activar la compressió per al conjunt de dades de les VM mentre mantenim la compressió desactivada per al conjunt de dades d’emmagatzematge de còpia de seguretat dels fitxers de còpia de seguretat de VM i video. Ja estan comprimits, estalviant així recursos valuosos del servidor. Per crear els conjunts de dades, que es diu pool en ZFS zfs crea &lt;zpool_name&gt;/&lt;zfs_dataset_name&gt; El zpool_name és el que hem creat zfspool1 i el dataset el que volem crear, next per exemple, per a les dades de Nextcloud. Els conjunts de dades s’han de muntar en un directori abans que es puguen utilitzar. De manera predeterminada, un nou conjunt de dades o conjunt de dades zfs es munta al directori arrel. zfs set mountpoint=/mnt/zfs-vm zfspool1/next En el nostre cas li direm zfspool1 i del tipus RAID0 que és no string Per habilitar la compressió per al conjunt de dades zfs set compression=on zfspool1/next En combinar l’agrupació ZFS amb una compartició NFS, podem crear un emmagatzematge compartit amb funcions ZFS completes. Creant d’aquesta forma un emmagatzematge compartit flexible. Podríem muntar cada volum en la màquina virtual que el vol fer servir, sense falta del Truenas, pero haguérem de donar-li RAM suficient a cada màquina per a gestionar ZFS i les tasques de còpia de seguretat serien més laborioses. Millor centralitzar-ho tot en una VM dedicada a aquestes labors com Truenas. Si tinguerem soles el Truenas, tots els passos de la creació de la pool ens els podríem haver estalviat, es pot fer mitjançant la seua GUI. Ara l’afegim, al nostre servidor proxmox per passar-lo després a la VM Truenas. En centre de dades -&gt; emmagatzemament -&gt; ZFS -&gt; afegir L’ID és el nom que li volem donar que siga únic en el servidor. Elegim el zfspool1, el mateix que en el que l’hem creat. Afegir disc ZFS Imatge 6.1: (Afegir pool a proxmox) Ara l’afegirem a Tuenas. En la màquina virtual afegim el disc ZFS que acabem de crear. L’afegim d’aquesta forma virtIO block perquè el disc siga una unitat física real, no un disc dur virtual. Les dades estan en un disc real i no en una imatge de màquina virtual, i es pot accedir a elles encara que la VM ja no estiga. No em deixa posar més de 93GiB, diu que el disc supera l’espai, coses de ZFS que es fa una partició reservada. Afegir el disc a freeNas en mode pass Imatge 6.2: (Disc afegit a Truenas) 6.1.6 Configurar el nou disk Dins de Truenas, anem a storage -&gt; diks per configurar-lo Truenas configurar el nou disc Li podríem canviar les opcions d’energia del disc, encriptar … Configurem el HD I ho salvem En principi és indiferent qui cee els volums ZFS, en el proposat abans els fa Proxmox i després el muntem per compartir-lo en Truenas, pero no sé si és la forma més correcta de fer-ho. Ho vaig fer al principi i és la solució que vaig trobar, per a la resta de configuracions d’espais de dades i zvol compartits no afecta. Per al tercer disc SATA el que farà de Còpia de seguretat Total, repetirem el procés, pero tot amb la GUI de Truenas. En la pràctica seria soles amb un disc SATA, pero per fer la demostració, ho farem en dos discs com abans, una vegada fet el volum, la resta és com si fora soles una unitat física. A partir d’aci, crearíem la primera pool igual que l’apartat posterior, pero soles amb el disc ada0. 6.1.7 Passthrough disc físic a màquina virtual (forma correcta) Passthrough disc físic a màquina virtual (VM) Per fer-ho d’aquesta segona forma, hem afegit dos discs més a proxmox per KVM /dev/vdf /dev/vdg Afegir dos discs nouns a Proxmox La idea era que aci tindrien soles el tercer SATA, pero afegim dos per fer el RAID per Truenas que no hem fet abans. Primer hem de veure les característiques dels discs a afegir amb lshw root@proxmox:~# lshw -class disk -class storage *-scsi description: SCSI storage controller product: Virtio block device vendor: Red Hat, Inc. physical id: 0 bus info: pci@0000:04:00.0 version: 01 width: 64 bits clock: 33MHz capabilities: scsi msix pm pciexpress bus_master cap_list configuration: driver=virtio-pci latency=0 resources: irq:22 memory:f9a00000-f9a00fff memory:fe400000-fe403fff *-virtio2 description: Virtual I/O device physical id: 0 bus info: virtio@2 logical name: /dev/vda size: 40GiB (42GB) capabilities: gpt-1.00 partitioned partitioned:gpt configuration: driver=virtio_blk guid=f2e35e7d-3cde-4753-be5c-f7e181a4f0fe logicalsectorsize=512 sectorsize=512 .... *-scsi description: SCSI storage controller product: Virtio block device vendor: Red Hat, Inc. physical id: 0 bus info: pci@0000:0f:00.0 version: 01 width: 64 bits clock: 33MHz capabilities: scsi msix pm pciexpress bus_master cap_list configuration: driver=virtio-pci latency=0 resources: irq:23 memory:f8600000-f8600fff memory:fd000000-fd003fff *-virtio11 description: Virtual I/O device physical id: 0 bus info: virtio@11 logical name: /dev/vdf size: 50GiB (53GB) configuration: driver=virtio_blk logicalsectorsize=512 sectorsize=512 *-scsi description: SCSI storage controller product: Virtio block device vendor: Red Hat, Inc. physical id: 0 bus info: pci@0000:10:00.0 version: 01 width: 64 bits clock: 33MHz capabilities: scsi msix pm pciexpress bus_master cap_list configuration: driver=virtio-pci latency=0 resources: irq:23 memory:f8400000-f8400fff memory:fce00000-fce03fff *-virtio12 description: Virtual I/O device physical id: 0 bus info: virtio@12 logical name: /dev/vdg size: 50GiB (53GB) configuration: driver=virtio_blk logicalsectorsize=512 sectorsize=512 Ens interessen els dos últims virtio11 virtio12 de 53 GB pero al ser un disc virtual en realitat no apareix el model del producte ni el guid, els afegirem per dev/vdf i g com a SATA2 i 3, millor seria afegir-los per by-id. root@proxmox:~# qm set 102 -sata2 /dev/vdg update VM 102: -sata2 /dev/vdg root@proxmox:~# qm set 102 -sata3 /dev/vdf update VM 102: -sata3 /dev/vdf Ara ja apareixen en la VM de truenas com SATA2 i 3. Estan connectats a Truenas, sense estar manipulats ni muntats en Proxmox. L’hem d’afegir de tipus SATA perquè es crega que és un disc real, si el posem com scsi el detecta com a qcow, i no el podem enganyar. Afegir discs directament Truenas En entrar en la GUI, en discs, ens apareixen els nous dos discs ada1 ada2, el ada0 és el zfspool1 del primer pas. Discs des de Truenas Ara crearem el pool amb aquests dos discs. Guia Storage Configuration La pool que es veu datatruenas és la que vam crear en el volum zfspool1 per les dades de Nextcloud i còpia de seguretat que es detalla com crear-les i compartir-les en els apartats corresponents a aquestes VM. Una vegada ja tenim els nous discs afegits, anem a piscines (pools) per crear la nova unitat amb afegir. Anem a pool per crear una nova Seguim les instruccios del guiat, crear piscina nova Elegim els discs que volem per a la nova pool Seleccionem els discs que volem, els dos nous que acabem d’afegir i que no tenen format, El ada0 no apareix, perquè ja té creada la pool. Crea pool nova Els passem a la dreta i elegim ratlla (striping) per fer RAID 0, ens apareix un missatge d’advertència dient que no té redundància de dades .. L’acaptem Elegim tipus ratlla Apareix una altra advertència, l’acceptem, cliquem força Forcem la creació RAID 0 Li donem un nom i acceptem Confirmem que estem segurs I el crearia. Apareix una altra pool, còpia de seguretat total, on podem crear Zvol i dades. Nova pool A partir d’aci és com en el cas anterior, pero d’una forma més correcta de fer-ho. La nova pool, és indiferent que siga formada per un sol disc o per dos, en el cas real seria de soles un disc per aquesta pool BackupTotal. Imatge 6.3: (Pool backup Total) Ja tindriem el tercer SATA per fer alli les còpies de seguretat de tot el sistema. 6.1.8 Crear dataset Com a exemple, crearem un dataset per a fer les còpies de seguretat de les VM, guardar les ISO dels sistemes a instal·lar, les imatges LCX … El dataset vindria a ser aproximadament anàleg a un sistema de fitxers muntat estàndard, que podem compartir i on podem crear capetes per a compartir amb NFS, SMB… Sera la carpeta d’administració que després compartirem en Proxmox on estarà centralitzat tots els fitxers d’imatges.. En storage -&gt; pool, sobre datatruenas que és el disc ada0 (zfspool1), afegim un nou Dataset. El next és el de Nextcloud, explicat en el seu apartat. ADD -&gt; Add dataset sobre datatruenas Afegir pool d’administració de Proxmox En la configuració li donem el nom del recurs, una xicoteta descripció. En el nivell de compressió li donem a off, no té sentit en aquest volum, ja que el que guardarem en ell, ja està comprimit, ÏSO, còpies de seguretat de VM, etc. Configuracio de la pool Imatge 6.4: (Crear Dataset) Quedaria compartir-lo en NFS 6.1.8.1 Crear Zvol Una altra opcio seria afegir un Zvol, és tipus block, com si ferem un disc real de mida fixa, al que podem fer partricions, LVM .. i formatar les particions que creem en el format que vulguem, EXT4,NTFS …, Pero mante les caracteristique de ZFS, instanatanies, restauracio, control d’errors per a Tuenas. Al sistema client, seria un disc dur normal, aquest recurs l’hem de compartir, no caom a seveis de xarxa normails, sinó amb iSCSI. iSCSI és un protocol de xarxa que permet posar els discs durs en una caixa NAS o servidor, però a l’ordinador apareixen com si estaguera connectat localment. En linux hem de fer una xicoteta configuracio, exemple en ubuntu, per al que el detecte, pero a partir d’aci és com un disk normal. Té les seues ventages. Creem un volum d’aquest tipus dins del dataset Backup, per tindre-ho ordenat, per fer proves, l’unic inconveniesnt és que les VM que es creen aci, no es poden migrar a un altre servidor proxmox en calent. Per exemple, es poden crear diferents perticions o volums Zvol i asignar cadascuna a un usuari o host, i tindria el seu volum aillat de la resta dels companys. El més recomanat seria fer un volum per a les imatges de les ISO, LXC… i un altre per les còpies de seguretat de les VM del servidor Proxmox1, per si en el futur es fa un cluster, i volem tindre centralitzat les ISO, i, per una altra banda, les còpies de seguretat de les VM de cada Proxmox. Pero farem un sol Zvol de moment. Per afegir aquest volum, fem el mateix que abans, pero damunt de backup, i en comptes d’afegir un altre dataset, afegim un Zvol. BackupProxmox1 :::{.rmdwarn data-latex=“{Perill}” Després en la GUI, Backup ho tradueix per Còpia de seguretat Proxmox1, pero la referència al recurs és BackupProxmox1, igual que nesxt que és la de Nextcloud la tradueix com a proxim. Lia més que ajudar. ::: Aci li donarem un nom, i la capacitat que hem d’especificar en GiB perquè ho entenga. Podem forçar que es limite al tammany, pero no es recomana, comença a donar problemes quan arriba al 80%. El nivell de compressió el deixem com està, desactivat. Zvol de Backup per a Prozmox El tindriem que compartir com a ISCSI i en proxmox afegir-lo com a ISCSI sobre ZFS ISCSI over ZFS Tindriem que configurar la comparticio ISCSI, hem de definir portal, bancs, usuari. Queda pendent per provar. El cas és que en utilitzar el dataset backup compartit per NFS en proxmox, dins d’ell està el Zvol backupProxmox1 i fer còpies de seguretat de VM i la LXC que hem baixat. Backup proxmox Després en la pool de Truenas, pareix que ha fet les carpetes i guardat les dades dins el Zvol BackupProxmox1, dins de Backup soles està el Zvol Truenas backup Pero si editem el recus Backup compartit per NFS estan les carpetes de Proxmox, les definides en afegir-lo, dump per a les còpies de seguretat de les VM, imatges per a les ISO … En l’espai ocupat correcte. Directoris dins de Backup NFS En fer la replia posteriorment, es replica el Zvol, no ho tinc encara massa clar el que està pasant. 6.1.8.2 Activar NFS Per activar el protoclo de comparticio, perimer l’hem d’abilitar. Anem a services, i habilitem la compartició NFS, i marquem el quadre de posar en funcionament en arrancar. Per poder compartir el recurs per xarxa, estan tots protocols per fer-ho, SMB, TFTP… Pero com totes les màquines són Linux, prefereix fer-ho d’aquesta manera. També es podria fer amb iscsi que on la host client es creu que es un disc real. Queda pendent d’investigar el funcionament. Encendre NFS 6.1.8.3 Compartir el recurs En sharing -&gt; NFS, afegim nou recurs i el configurem. Primer elegim el directori a compartir, el dataset backup (dins d’aquest tenim el Zvol backupproxmox1) en datatruenas, elegim all dir perquè el client puga elegir el subdirectori que vulga o tots esl que hi haja. Li donem a opcions avançades, i li donem permisos soles al root per a manipular-lo, i cap altre usuari el pot mapar. En Networks li diguem la xarxa on estarà disponible, posteriorment el pasarem a la xarxa VLAN 1 172.16.0.0/24 i la IP fixa de proxmox en aquesta xarxa. En un futur, configurarem primer el DNS de pfSense per posar els noms de les màquines i no les IP. D’aquesta forma quan configurem les VLAN, i la d’administració, haurem de tornar a configurar totes les IP o posar ja el nom real de la màquina de totes les configuracions. Comparticio NFS dataset backup En proxmox l’afegiriem de la mateixa forma que vam fer en el recurs compartit per el host al principi. L’hem anomenat backup llista espais emmagatzenament proxmox La configuracio en l’apartat de Poxmox. "],["politaca-de-volums.html", "6.2 Politaca de volums", " 6.2 Politaca de volums S’hauria de pensar i veure l’espai total final de discs que tenim. Suposant que tenim 3 SATA de 4 T repartiriem l’espai dels 2 primers 8 Gib de forma que puguerem fer espais iguals en l’altre de 4 T. El zvol de dades de nextcloud 2 T -&gt; NFS -&gt; Nextcloud Backup VM 0.5 T (en estar comprimides, i per la quantitat de VM i LXC mes les ISO … Seria suficient) -&gt; NFS -&gt; Proxmox La resta per al Zvol de video, dins d’aquest un dataset datasetVideo que té tot l’espai i dins del dataset de video, un Zvol per als videos editats per guardar (1.5 T) en un dataset datasetVideosEditats per assegurar que la grandària del disc és el mateix que el de backup total, per poder compartir en Zoneminder i Nextcloud a l’hora, la resta de l’espai per als videos en brut. datasetVideo -&gt; NFS -&gt; Zonaminder &amp; Nextcloud datasetVideos Edoitats -&gt; Nextcloud (és redundant, pero per compartir amb membres dels grups no video), es pot fer servir tambe el truenas com un plex per compartir recursos multimedia, per si es vol compartir d’aquesta forma al projector que te la sala per promocions o TV. D’aquesta forma podem fer backups de la part de Nextcloud total, de la part administrativa total, i de la de videos editats tambe. I si els videos no editats omplen l’espai reservat per a ells (el mes normal amb el temps), no desbarate la resta de dades. "],["configuracio-del-disc-backup-total.html", "6.3 Configuracio del disc Backup Total", " 6.3 Configuracio del disc Backup Total En el tercer Sata. És on faria les còpies de seguretat de tot. Editariem les opcions d’energia del disc, ja que la major part del temps estara parat i mimimitzen el seu desgast. Soles es posara en martxa a l’hora de fer les sincronitzacions, una vegada al dia en dades de Nextcloud i video, i una al mes en les d’administracio. En la seccio de disc, editariem el SATA 3 ho simulem en el ada1 backup total, mirem la configuracio Seleccionem el disc Editem la configuracio Editem la configuracio Cambiem la configuracio HDD en espare, de sempre ences a la minima Configuracio en espera Gestio avançada d’energia el nivell 1 ús minim d’energia en espera. Gestio d’energia 6.3.1 Treballs de sincronitzacio Primer fariem els datasets, fhe provat el de nextcloud que és xicotet i el d’administracio, la resta seria igual. En la pool backupTotal creem una nou dataset Backup2 pel d’administracio i Backupnextcluddades per a nextcloud, com sempre, amdb la seguent configuracio. El Backup2 ja l’he replicat, es veu que no està buit, ho farem en el de Nextcloud que no tarde tant. Datasets Backup Total per rèpliques Primes hem de fer una snapshot, les rèpliques es fan sobre elles, si no fem, la faria en definir la tasca. En periodic snapshot creem una nova. Elegim la que volem datatruenas/next La vida de la snapshot, després l’esborra, 2 setmanes La periocitat diaria a les 12 PM Snapdhot dataset Quedaria samapxhot resum Fem la rèplica Anem a task (treballs) es on podem coonfigurar les tasques de Truenas, rsync, instantanies… En replication Task afegim una nova Elegim l’origen datatruenas/next El desti BackupTotal/BackupNextcloudDades seguent Replica Nextcloud Que ho faça per la programacio o soles una vegada, per la programacio La vida de les instantanies en el desti, el mateix que en l’origen, es borren amb la mateixa periodicitat, 2 setmanes. La programacio de quant fer-les, la mateixa que les snpshot pero una hora després. Replica Nextcloud Ja estaria feta la rèplica Replica Nextcloud Per restaurar tindriem que donar-li a restore i tot en ordre altra vegada. Podem veure que la mida de la rèplica i l’original, i el sistema de fitxers o Zvol és el mateix que l’original, encara que soles hem definit un dataset en Backup Total. Compara replica i l’original Imatge 6.5: (Crear replica) Truenas pot fer moltes coses mes com Instal·lar VM en jail, Webdav, servidor OpenVPN … Propies i de la comunitat, simplement instal·lant un connector i ho configura, vaig provar nextcloud, pero millor en una VM per a ella soles si tenim de base proxmox, o la posibilitat de fer la base Truenas i la resta de serveis dins d’ella. connectors Truenas "],["nextcloud.html", "Capitol 7 Nextcloud", " Capitol 7 Nextcloud Per la fase de proves hem creat la VM partint de la imatge oficial Nextcloud VM on provarem les configuracions del servidor, xarxa, disc … Pero una vegada fase de producció, partirem d’un Ubuntu server 20.04 i els scripts que proporciona nextcloud/vm del repositori Github (Guia de com fer-ho), que té un certificat de seguretat A+ i optimitza diverses funcions. Primer fem una VM amb Ubuntu 20.04 server, clonem el repositori i executem els scripts com diu la Gia comentada. Ho he provat en Virtual Box i no te mes complicacions. wget https://raw.githubusercontent.com/nextcloud/vm/master\\ /nextcloud_install_production.sh sudo bash nextcloud_install_production.sh sudo bash /var/scripts/nextcloud-startup-script.sh Per fer una instal·lacio manual tenim aquesta altra Giua "],["importar-imatge-.html", "7.1 Importar imatge .vmdk", " 7.1 Importar imatge .vmdk Per provar coses noves, importarem una imatge .vmdk des d’una .ova a Proxmox en format .RAW, seguint els passos descrits en Install .ova Image on Proxmox Descarreguem la VM oficial i la descomprimim. Una vegada descomprimit l’arxiu .ova copiem l’arxiu per scp a proxmox en /tmp enkidu@enkidu:~$ tar -xvf Virtual_Appliance_Debian.ova Official-Nextcloud-VM.ovf Official-Nextcloud-VM.mf Official-Nextcloud-VM-disk1.vmdk Official-Nextcloud-VM-disk2.vmdk Copiem la imatge a Proxmox enkidu@enkidu:~$ scp Official-Nextcloud-VM-disk1.vmdk root@192.168.122.2:/tmp/ root@192.168.122.2&#39;s password: Official-Nextcloud-VM-disk1.vmdk 100% 1714MB 13.8MB/s 02:03 Des de la consola de Proxmox, creem una nova màquina i importem el disc de la VM $ qm importdisk 105 /tmp/Official-Nextcloud-VM-disk1.vmdk vmZFS -format raw importing disk &#39;/tmp/Official-Nextcloud-VM-disk1.vmdk&#39; to VM 105 ... transferred 0.0 B of 40.0 GiB (0.00%) transferred 409.6 MiB of 40.0 GiB (1.00%) transferred 819.2 MiB of 40.0 GiB (2.00%) transferred 1.2 GiB of 40.0 GiB (3.00%) transferred 1.6 GiB of 40.0 GiB (4.00%) transferred 39.2 GiB of 40.0 GiB (98.00%) transferred 39.6 GiB of 40.0 GiB (99.00%) transferred 40.0 GiB of 40.0 GiB (100.00%) transferred 40.0 GiB of 40.0 GiB (100.00%) Successfully imported disk as &#39;unused0:vmZFS:vm-105-disk-1&#39; nextcloud disc importat Després esborrem el disc que havíem creat per crear la VM, disk-0 de 32G i seleccionat el disc nou l’afegim, en aquests moments esta sense ús, l’he canviat també al tipus VirtlO. Afegir HD nou i activar-lo Si per una d’aquestes mo arranca, mirar en opcions de la VM l’ordre d’arrancada, pot que siga ISO, NET, HD, e intenta arrancar per xarxa, es canviar l’ordre com ho faríem en la bios, i posar el HD endavant de per la xarxa. Una vegada arrancada es user: ncadmin passwors: nextcloud i ens demana que canviem el pasword de l’admin, el teclat … No es una versió de producció, falten algunes característiques millorades, que es poden instal·lar després dels scripts explicats mes avall, pero per a fer les proves ens val. "],["volum-de-dades-nfs.html", "7.2 volum de dades NFS", " 7.2 volum de dades NFS Una vegada arrancada la VM, li afegirem el recurs d’espai on volem que es guarden les dades de l’usuari. Sera un recurs compartit per Turenas en NFS Instal·lem en el la VM Nextcloud el client de NFS sudo apt -y install nfs-common Actualitzem el domini. sudo nano /etc/idmapd.conf I posem el nostre. Domini en Nextcloud Ara muntem el volum que estem servint en Truenas En la secció de Truenas s’explica com compartir recursos, aci soles el muntarem. Directori servit per Tuenas /mnt/datatruenas/next Primer creem el directori en mnt on muntar el recurs i li donem propietari, li donarem els mateixos permisos que te el que per defecte crea, ncdata. root@nextcloud:/mnt# ls -la total 12 drwxr-xr-x 3 root root 4096 Oct 21 2021 . drwxr-xr-x 20 root root 4096 Oct 21 2021 .. drwxrwx--- 6 www-data www-data 4096 May 28 17:52 ncdata root@nextcloud:/mnt# mkdir -p /mn/datanext root@nextcloud:/mnt# chown www-data:www-data /mnt/datanext root@nextcloud:/mnt# chmod 770 /mnt/datanext root@nextcloud:/mnt# ls -la total 16 drwxr-xr-x 4 root root 4096 May 28 18:51 . drwxr-xr-x 20 root root 4096 Oct 21 2021 .. drwxrwx--- 2 www-data www-data 4096 May 28 18:49 datanext drwxrwx--- 6 www-data www-data 4096 May 28 17:52 ncdata Ara muntem el volum root@nextcloud:/mnt# mount -t nfs Truenas.inestable.dedyn.io:/mnt/dataTruenas/next\\ /mnt/datanext/ Si tot va bé, l’afegim a /etc/fstab perquè el munte automàticament en l’arracada. Comprovem que anat tot be root@nextcloud:/mnt# df -h Filesystem Size Used Avail Use% Mounted on udev 3.8G 0 3.8G 0% /dev tmpfs 778M 1.2M 777M 1% /run /dev/mapper/ubuntu--vg-ubuntu--lv 39G 5.9G 31G 17% / tmpfs 3.8G 16K 3.8G 1% /dev/shm tmpfs 5.0M 0 5.0M 0% /run/lock tmpfs 3.8G 0 3.8G 0% /sys/fs/cgroup /dev/vda2 976M 103M 806M 12% /boot /dev/loop1 56M 56M 0 100% /snap/core18/2128 /dev/loop3 62M 62M 0 100% /snap/core20/1169 /dev/loop5 68M 68M 0 100% /snap/lxd/21545 /dev/loop7 56M 56M 0 100% /snap/core18/2409 /dev/loop8 62M 62M 0 100% /snap/core20/1494 /dev/loop0 45M 45M 0 100% /snap/snapd/15904 /dev/loop6 68M 68M 0 100% /snap/lxd/22753 tmpfs 778M 0 778M 0% /run/user/1000 Truenas.inestable.dedyn.io:/mnt/dataTruenas/next 30G 128K 30G 1% /mnt/datanext Modifique el /etc/fstab per fer persistent aquest directori, afegint la linea Truenas.inestable.dedyn.io:/mnt/dataTruenas/next\\ /mnt/datanext nfs auto,nofail,noatime,nolock,intr,tcp,actimeo=1800 0 0 editem fstab Ara hem de passar els fitxers creats en ncdata al nou directori i configurar en /var/www/nextcloud/config/config.php de nextcloud el nou directori. Realment els arxius importants son el .htaccess i .ocdata pero copiarem també el index i els logs root@nextcloud:/mnt# ls -hla /mnt/ncdata/ total 40K drwxrwx--- 6 www-data www-data 4.0K May 28 17:52 . drwxr-xr-x 4 root root 4.0K May 28 18:51 .. drwxr-xr-x 4 www-data www-data 4.0K May 28 17:05 2585289c-72e1-103c-8cae-9fbe74979792 drwxr-xr-x 8 www-data www-data 4.0K May 28 16:38 appdata_ocru8ytkzh2p -rw-rw-r-- 1 www-data www-data 0 Oct 21 2021 audit.log -rw-r--r-- 1 root www-data 542 Oct 21 2021 .htaccess drwxr-xr-x 4 www-data www-data 4.0K May 28 16:38 incitato -rw-rw-r-- 1 www-data www-data 0 May 28 16:35 index.html -rw-rw-r-- 1 www-data www-data 0 May 28 16:35 .ocdata -rw-rw-r-- 1 www-data www-data 11K May 28 16:35 updater.log drwxrwxr-x 4 www-data www-data 4.0K May 28 16:35 updater-oc Per poder fer aço, primer hem de donar permisos al root en el recurs compartit en Truenas. Hem de posar en el Truenas permís per al root per poder copiar els arxius, després ho canviarem a www permis temporal a root Apaguem el servei apache en VM Nextcloud, perquè no canvie els arxius mentre els estem copiant per si de cas. sytemctl stop apache2 cp -R /mnt/ncdate/ /mnt/datanext/ Hem de canviar el directori que apunta les dates en el fitxer de configuració root@nextcloud:/home/ncadmin# nano /var/www/nextcloud/config/config.php i canviar datadirectory pel nou ‘datadirectory’ =&gt; ‘/mnt/datanext’, Canvi de directori de dades Copiem el .htaccess i li donem els mateixos permisos que tenia root@nextcloud:/mnt# cp ncdata/audit.log index.html .ocdata /mnt/datanext/ root@nextcloud:/mnt# ls -la /mnt/ncdata/.htaccess -rw-r--r-- 1 root www-data 542 Oct 21 2021 /mnt/ncdata/.htaccess Li posem els mateixos permisos root@nextcloud:/mnt# ls -la ncdata/ total 40 drwxrwx--- 6 www-data www-data 4096 May 28 20:30 . drwxr-xr-x 4 root root 4096 May 28 18:51 .. drwxr-xr-x 4 www-data www-data 4096 May 28 17:05 2585289c-72e1-103c-8cae-9fbe74979792 drwxr-xr-x 8 www-data www-data 4096 May 28 16:38 appdata_ocru8ytkzh2p -rw-rw-r-- 1 www-data www-data 0 Oct 21 2021 audit.log -rw-r--r-- 1 root www-data 542 Oct 21 2021 .htaccess drwxr-xr-x 4 www-data www-data 4096 May 28 16:38 incitato -rw-rw-r-- 1 www-data www-data 0 May 28 16:35 index.html -rw-rw-r-- 1 www-data www-data 0 May 28 16:35 .ocdata -rw-rw-r-- 1 www-data www-data 10870 May 28 16:35 updater.log drwxrwxr-x 4 www-data www-data 4096 May 28 16:35 updater-ocru8ytkzh2p root@nextcloud:/mnt# cd ncdata/ root@nextcloud:/mnt/ncdata# cp audit.log index.html .ocdata /mnt/datanext/ root@nextcloud:/mnt/ncdata# ls -la /mnt/datanext/ total 19 drwxrwx--- 3 root www-data 7 May 28 20:48 . drwxr-xr-x 4 root root 4096 May 28 18:51 .. drwxr-xr-x 3 www-data www-data 3 May 28 20:43 appdata_ocru8ytkzh2p -rw-r--r-- 1 root www-data 0 May 28 20:48 audit.log -rw-r--r-- 1 www-data www-data 542 May 28 20:27 .htaccess -rw-r--r-- 1 root www-data 0 May 28 20:48 index.html -rw-r--r-- 1 root www-data 0 May 28 20:48 .ocdata root@nextcloud:/mnt/ncdata# cd /mnt/datanext/ root@nextcloud:/mnt/datanext# chown www-data audit.log index.html .ocdata root@nextcloud:/mnt/datanext# chown root .htaccess root@nextcloud:/mnt/datanext# chmod 664 audit.log index.html .ocdata root@nextcloud:/mnt/datanext# ls -la total 19 drwxrwx--- 3 root www-data 7 May 28 20:48 . drwxr-xr-x 4 root root 4096 May 28 18:51 .. drwxr-xr-x 4 www-data www-data 4 May 28 20:50 appdata_ocru8ytkzh2p -rw-rw-r-- 1 www-data www-data 0 May 28 20:48 audit.log -rw-r--r-- 1 root www-data 542 May 28 20:27 .htaccess -rw-rw-r-- 1 www-data www-data 0 May 28 20:48 index.html -rw-rw-r-- 1 www-data www-data 0 May 28 20:48 .ocdata Canviem el propietari en Truenas per al recurs compartit per a www Canviar de propietari a www Entrem en Nextclown com a usuari i veiem que es crea la seua carpeta en el nou directori de dades root@nextcloud:/mnt/datanext# ls -la total 20 drwxrwx--- 4 root www-data 8 May 28 21:03 . drwxr-xr-x 4 root root 4096 May 28 18:51 .. drwxr-xr-x 5 www-data www-data 5 May 28 21:03 appdata_ocru8ytkzh2p -rw-rw-r-- 1 www-data www-data 0 May 28 20:48 audit.log -rw-r--r-- 1 root www-data 542 May 28 20:27 .htaccess drwxr-xr-x 4 www-data www-data 4 May 28 21:04 incitato -rw-rw-r-- 1 www-data www-data 0 May 28 20:48 index.html -rw-rw-r-- 1 www-data www-data 0 May 28 20:48 .ocdata Ara creem una carpeta en l’usuari i en el GUI la crea i en el directori tambe apareix. Afegim una carpeta de prova root@nextcloud:/mnt/datanext# ls -la incitato/files/ total 2 drwxr-xr-x 3 www-data www-data 3 May 28 21:09 . drwxr-xr-x 5 www-data www-data 5 May 28 21:09 .. drwxr-xr-x 2 www-data www-data 2 May 28 21:09 carpetaProva En afegir-lo com a recurs per xarxa, en fer les còpies de seguretat de les VM des de Proxmox, aquest recurs no entra en el Backup, soles guarda la VM. còpia de seguretat "],["gestió-dusuaris-openldap.html", "7.3 Gestió d’usuaris openLDAP", " 7.3 Gestió d’usuaris openLDAP La gestió de l’usuari es farà per LDAP, Hem configurat una VM que sera el servidor de LDA. Nexcloud té en la GUI del administrador un connector que ens facilita aquesta feina. Primer hem d’activar el mòdul LDAP, en l’usuari Administrador, anem a aplicacions, i baix de tot prenem el botó d’activació. Activar el plugin LDAP Nextcloud Després anem a paràmetres i configurem que busque en el nostre servidor LDAP.inestable.dedyn.io Donar el nom del nostre servidor detecta el port, ho fa automaticament usuari de cerca, com ho tenim configurat, realitzar cerques anonimes, no seria necesari. Detecta la Base, ho fa automaticament. Configuracio base LDAP De moment estem en una configuracio basica de LDAP, hem creat un usuari per provar que funciona, més avant, crearem una estructura real de LDAP, de moment elegim posisxAccount Configuració d’usuaris Finalment, els grups que tenen dret a accedir, eligiriem nextcloud, que es el grup on afegirem els usuaris amb acces. Configuracio LDAP Grups Entrem en un usuari que no està en el fitxer /etc/passwd root@nextcloud:/home/ncadmin# getent passwd root:x:0:0:root:/root:/bin/bash daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin bin:x:2:2:bin:/bin:/usr/sbin/nologin sys:x:3:3:sys:/dev:/usr/sbin/nologin sync:x:4:65534:sync:/bin:/bin/sync games:x:5:60:games:/usr/games:/usr/sbin/nologin man:x:6:12:man:/var/cache/man:/usr/sbin/nologin lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin mail:x:8:8:mail:/var/mail:/usr/sbin/nologin news:x:9:9:news:/var/spool/news:/usr/sbin/nologin uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin proxy:x:13:13:proxy:/bin:/usr/sbin/nologin www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin backup:x:34:34:backup:/var/backups:/usr/sbin/nologin list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin irc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin systemd-network:x:100:102:systemd Network Management,,,:/run/systemd:/usr/sbin/nologin systemd-resolve:x:101:103:systemd Resolver,,,:/run/systemd:/usr/sbin/nologin systemd-timesync:x:102:104:systemd Time Synchronization,,,:/run/systemd:/usr/sbin/nologin messagebus:x:103:106::/nonexistent:/usr/sbin/nologin syslog:x:104:110::/home/syslog:/usr/sbin/nologin _apt:x:105:65534::/nonexistent:/usr/sbin/nologin tss:x:106:111:TPM software stack,,,:/var/lib/tpm:/bin/false uuidd:x:107:112::/run/uuidd:/usr/sbin/nologin tcpdump:x:108:113::/nonexistent:/usr/sbin/nologin landscape:x:109:115::/var/lib/landscape:/usr/sbin/nologin pollinate:x:110:1::/var/cache/pollinate:/bin/false sshd:x:111:65534::/run/sshd:/usr/sbin/nologin systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin ncadmin:x:1000:1000:ncadmin:/home/ncadmin:/bin/bash lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false postgres:x:112:118:PostgreSQL administrator,,,:/var/lib/postgresql:/bin/bash incitato:x:1001:1001:,,,:/home/incitato:/bin/bash _rpc:x:113:65534::/run/rpcbind:/usr/sbin/nologin statd:x:114:65534::/var/lib/nfs:/usr/sbin/nologin root:x:0:0:root:/root:/bin/sh nobody:x:65534:65534:nobody:/:/usr/sbin/nologin L`usuari ramiro creat en ldap, no apareix com a un del sistema, soles es podra logar en Netxcloud, no hem tocat la configuracio del sistema base. Usuari LDAP en Nextcloud "],["certificat-lets-encrypt.html", "7.4 Certificat Let’s Encrypt", " 7.4 Certificat Let’s Encrypt Farem el certificat en Let’s Encrypt Seguint els pasos que ens diu ho farem amb certbot Mitjançant snap sudo snap install core; sudo snap refresh core sudo snap install --classic certbot Fem un enllaç a /usr/bin/cerbot per asegurar-nos que es pot executar sudo ln -s /snap/bin/certbot /usr/bin/certbot Per obtenir un certificat i que Certbot edite la configuració d’apache automàticament, activant l’accés HTTPS en un sol pas. sudo certbot --apache El crea una carpeta dins de /etc on guarda els certificats root@inestable:/etc/letsencrypt/live/inestable.dedyn.io# ls cert.pem chain.pem dhparam.pem fullchain.pem privkey.pem README Els certificats de Let’s Encrypt només són vàlids durant noranta dies. Verificació de la renovació automàtica de Certbot, ja s’encarrega afegint un script de renovació a /etc/cron.d per verificar que funciona. sudo certbot renew --dry-run Ara simplement dins de /etc/apache2/sites-enables/inestable.dedyn.io afegim en e’apartat de virtualhost *:443 l’encriptat si no ho haguera fet automaticament. ### LOCATION OF CERT FILES ### SSLCertificateChainFile /etc/letsencrypt/live/inestable.dedyn.io/chain.pem SSLOpenSSLConfCmd DHParameters /etc/letsencrypt/live/inestable.dedyn.io/dhparam.pem Protocols h2 SSLEngine on SSLCertificateFile /etc/letsencrypt/live/inestable.dedyn.io/cert.pem SSLCertificateKeyFile /etc/letsencrypt/live/inestable.dedyn.io/privkey.pem SSLCACertificateFile /etc/letsencrypt/live/inestable.dedyn.io/chain.pem SSLVerifyClient none certificat let’s Encrypt Es faria de la mateixa forma per a Zoneminder, el servidor web o el sevei que volem tindre el certificat. Aquest procediment el vaig fer en la primera versio de les proves, en Virtual Box, on el servei de nextcloud estava directament en inestable.dedyn.io, per a fer-lo en subdominis em pareix que hem de crear un registre txt amb una clau que el dona al fer el certificat, per demostrar que tens control sobre el DNS. "],["configuracio-daplicacions-nextcloud.html", "7.5 Configuracio d’aplicacions Nextcloud", " 7.5 Configuracio d’aplicacions Nextcloud Ara soles quedaria la configuracio de les aplicacions dins de Nextcloud, com Collabora, Talk, afegir recursos externs (li afegirem la carpeta de videos editats de Truenas), Compartir carpetes per a grups … Seria anar a Aplicacions en l’administrador i seleccionar les que volem. Instal·lar aplicacions Nextcloud Es recomana per a TALK crear un altre recurs DNS i configurar el protocol STUN Session Traversal Utilities for NAT (STUN) permet que un dispositiu descobrisca la seua adreça IP pública. Si es descobreix l’adreça IP pública tant de la persona que truca com de la persona que truca, és possible establir una connexió directa entre la persona que truca i la persona que truca, que normalment es coneix com a trucada d’igual a igual. Seria cosa de configurar pfSense aquest port i tocar les Qos. Queda pendent "],["zoneminder.html", "Capitol 8 Zoneminder", " Capitol 8 Zoneminder Fulla oficialzoneminder Resenya VM zonaminder turnkey Documentacio oficial Per provar el servei de zoneminder, crec que no és necessari una VM dedicada. Amb un contenidor LXC tenim suficient, a més, tampoc és un servei molt intensiu d’ús de recursos del sistema. Per la forma que s’utilitzara, no es faran servir les opcions de reconeixement d’esdeveniments i detecció de moviment, és soles per gravar video o veure-ho en directe d’una forma senzilla. Es pot fer servir directament VCL per a veure les càmeres en directe, o connectar a la web de la camera, l’aplicació és soles per a facilitar l’ús. La forma de funcionament de les càmeres ip ens permet poder adoptar multitud d’opcions a l’hora de fer la captura de les imatges, des de poder desar-les en un servidor ftp, per samba… directament de la camera, sense fer ús de cap programa intermedi, a poder ser gestionades des d’alguna aplicació que és la que realitzara la captura, com OBS que ens permet retransmetre després en streaming o guardar. O podem habilitar altres aplicacions per poder veure en directe el que està passant, gravar-ho o veure arxius antics, es sembla molt a un NVDR, per això provarem aquesta tecnologia encara que es podem implementar amb convicció de diverses solucions sobre el mateix recurs de disc, per compartir de diferents formes segons les necessitats de l’usuari, i els permisos que els volem donar. Seguint els passos de la guia oficial Instal·lar debian base amb LAMB, aprofitem el contenidor que hem creat an Proxmox, el 106 LXC $ apt install apache2 mariadb-server php libapache2-mod-php php-mysql lsb-release gnupg2 Assegurem mariadb $ mysql_secure_installation Afegim el repositori de zoneminder a /etc/source.list $ echo &quot;deb https://zmrepo.zoneminder.com/debian/release-1.36 &quot;`lsb_release -c -s`&quot;/&quot;\\ &gt;&gt; /etc/apt/sources.list.d/zoneminder.list La clau, actualitzem i instal·lem $ wget -O - https://zmrepo.zoneminder.com/debian/archive-keyring.gpg | apt-key add - $ apt-get update &amp;&amp; apt install zoneminder=1.36.11-bullseye1 Hem d’habilitar els serveis $ systemctl enable zoneminder.service $ systemctl start zoneminder $ a2enconf zoneminder $ a2enmod rewrite headers expires $ service apache2 reload ::: {.rmdwarn data-latex=“{Perill}” Dona un error root@Zoneminder:~# systemctl start zoneminder Job for zoneminder.service failed because the service did not take the steps required by its unit configuration. See “systemctl status zoneminder.service” and “journalctl -xe” for details. ::: Per arreglar este error em de canviar en la configuracio /lib/systemd/system/zoneminder.service i descomentar #User=www-data Error zoneminder Hebilitem els moduls $ a2enmod cgi $ a2enmod zoneminder $ a2enmod rewrite Reiniciem # service apache2 reload Zoneminer primera configuracio Ja podem entrar en zoneminder per afegir les càmeres i configurar l’espai per a fer les còpies de seguretat "],["ldap-apache.html", "8.1 LDAP Apache", " 8.1 LDAP Apache Apache with LDAP authentication Apache Web Server and LDAP Necesitem els moduls d’apache Ara mira per configurar els permisos per accedir al recurs. Mirar LDAP d’accés a Apache ldap, authnz_ldap, proxy i proxy_http. Els activem a2enmod authnz_ldap proxy_http Configurem el servidor apache, en etc/apache/sites-avaible/el_nostre_ZM &lt;VirtualHost *:80&gt; &lt;Location /&gt; Order allow,deny Allow from all AuthName &quot;AuthRequired&quot; AuthType Basic AuthBasicProvider ldap AuthzLDAPAuthoritative on AuthLDAPURL &quot;ldap://ldap.inestable.dedyn.io:389/ou=People,DC=inestable, DC=dedyn,DC=io?cn?sub?(objectClass=inetOrgPerson)&quot; require valid-user require ldap-group CN=video,OU=Groups,DC=inestable,DC=dedyn,DC=io &lt;/Location&gt; &lt;/VirtualHost&gt; Tambe ho podem fer per zoneminder.conf, hem de canviar Seguint els consells dáquesta fulla Below /zm Alias: &lt;Directory /usr/share/zoneminder/www&gt; AuthName &quot;ZoneMinder Login&quot; AuthBasicProvider ldap AuthLDAPURL &quot;ldap://ldap.inestable.dedyn.io/ou=video,dc=inestable,dc=dedyn,dc=io&quot; Require valid-user php_flag register_globals off [...] Docu oficial Pendent de provar refinar permisos, grups … LDAP "],["vlan.html", "8.2 VLAN", " 8.2 VLAN Aquesta màquina tindrà accés a tres VLAN LA de video per connectar en les càmeres, que no tenen internet, la privada per poder els usuaris de video accedir a les funcions de Zonaminder i veure privadament en streaming assajos, apache sols servira aquesta web per la xarxa de VLAN privat, gravar…, i la d’admim que és per on farem el compartiment de l’espai NFS o iSCI 8.2.1 Configuracio de les targetes de xarxa Li posarem 3 una per a cada VLAN connectades al pont de l’Open vswitch vmbr1 configurem cadascuna de les VLAN amb el dhcp de pfSense, perque li done una IP estàtica, també a les càmeres que es compren, a més del DNS. Xarxa Zoneminder La net0 l’esborrarem o desactivarem quan estiga tot configurat, de moment és per la que estem administrant tots els serveis. "],["configuració-de-les-càmeres.html", "8.3 Configuració de les càmeres", " 8.3 Configuració de les càmeres Depenent la camera que es compre, es configurara el protocol que suporte, la majoria és RTSP, configuraren les IP de les càmeres com a fixes en la VLAN 30 i afegirem registre DNS per a cadascuna Cam1 cam2 … Si esten en la VLAN de video, les podem connectar a qualsevol programa que vulguem gastar, des de l’editor de la taula de llums, l’OBS per fer streamin, inclos el VLC per fer captures o veureo en directe. Es pot configurar en molte d’elles des del seu servidor web, que copien a un nas, servidor FTP… és una altra posibilitat per gestionar-les. La configuració de l’app en si és deixa pendent per mes avant, quat es tinga la cam física. Documentació user Zoneminder Vaig revisar la documentació oficial i ens deixa fer tot el que volem, Amb diversos modes de funcionament, el de reconeixement de moviments per detectar la presència de gent i registra els esdeveniments sospitosos, és el mes problematic en consum de CPU i memòria, pero realment no ens interessa, a no ser que es pose una camera per aquest propòsit. El més interessant és el de gravació directa, i vista directa, que no consumeix molts recursos, ja que la codificació de video la fa la mateixa camera. Des del mateix programa es poden visionar gravacions antigues, esborrar-les, descarregar-les … No obstant aixo. Muntarem aquest recurs d’espai també en Nextcloud com a recurs extern. Donarem permís al grup de video, amb carpeta compartida de grup. Per facilitar les coses, i que puguen compartir amb usuaris no del grup video, que vulga tindre accés a un recurs determinat. Podent configurar des d’aci els permisos de validesa de temps, poder o no descarregar … És molt flexible Nextcloud en aquest aspecte. Tambe disposa d’app ZoneMinder Client App per android. Pendent "],["espai-demmagatzematge.html", "8.4 Espai d’emmagatzematge", " 8.4 Espai d’emmagatzematge Aquest apart seria igual al que vam fer en Nextcloud, servir per NFS el dataset de video i muntar-lo en /mnt/video per a guardar alli les gravacions. "],["proves-pendents.html", "8.5 Proves pendents", " 8.5 Proves pendents Està planejat fer proves fent servir VLC com a servidor RTSP, més avant, si no arriben les càmeres. Exemple de servidor VLC l’escriptori per RSTP vlc -I rc screen:// --sout=&quot;#transcode{vcodec=h264,vb=400,scale=0.25,fps=10}\\ :rtp{dst=127.0.0.1,port=4444,sdp=rtsp://localhost:8080/test.sdp}&quot; Pendent "],["installació-de-wifi.html", "Capitol 9 Instal·lació de wifi", " Capitol 9 Instal·lació de wifi Es realitzara amb dues antenes UniFi, una en l’entrada del recinte, i l’altra en meitat de l’escenari, per donar bona cobertura al públic i els actors. Hi ha molts aparells per les parets i estructures metàliques que podrien afectar a la qualitat del senyal. 9.0.1 Potència de la connexió d’internet per donar servei Es requereix que l’amplada de banda de la connexió d’internet puga donar servei almenys a 92 espectadors, que és l’aforament del pati de butaques. S’ha de limitar l’amplada de banda que oferim als espectadors. En cas de dur a terme la reproducció en línia al mateix temps. Açò ho realitzem assegurant l’amplada de banda en la xarxa del circuit d’imatge Qos en el pfSense, per a la VLAN 30, en la seua configuracio, pero s’han de fer proves per veure el valor òptim. Es recomana contractar una connexió d’amplada de banda minima 300Mb. "],["muntatge.html", "9.1 Muntatge", " 9.1 Muntatge Antena, una d’aquestes Unifi, recomane U6 Professional, pero no sé en les parets tan grans que hi ha en la sala, valdrà un sol per a tot l’espai. Es pot posar un altre repetidor sol per l’entrada. La idea, és fer tres xarxes. Una per al públic, en accés a internet i al servidor web intern de promocions d’espectacles. Una altra que tinga accés a la xarxa de l’oficina i recursos del servidor. I la tercera, soles per al tràfic de video, sense accés a internet, per poder utilitzar camera del movil en espectacles. No per al video de les càmeres IP que aniran per cable, molt més fiable que el wifi. Les antenes proposades són PoE, les alimentarem amb el switch, pel cable de xarxa. Software i documentació, posar-lo en una VM, en el servidor. Paquet .deb, per instal·lar en la màquina que fem servir de servidor. min 2GB de Ram, 20 GB hd. Especificacions antena pro "],["com-fer-ho.html", "9.2 Com fer-ho", " 9.2 Com fer-ho Documentació unifi wifi en pfsense vlan wifi video configuracio Configuracio VLAN pública, sols internet Configurarem dos punts d’accés WiFi autònoms que funciona per si sol i que depenen d’un encaminador independent per a DHCP/encaminament a Internet. Per a això, utilitzarem punts d’accés WiFi autònoms amb Ubiquiti UniFi. La Ubiquiti UniFi AP-AC Lite admet una velocitat wifi de fins a 300 Mbps a 2,4 GHz i 867 Mbps a una freqüència de 5 GHz. Els productes UniFi d’Ubiquiti usen “controlador de xarxa UniFi”, que és un programari que s’executa al vostre ordinador/servidor per gestionar diversos punts d’accés i configuració/provisionalment. És necessari per al subministrament per primera vegada del nostre punt d’accés UniFi, però no és necessari que s’execute constantment, ja que tota la configuració s’emmagatzema al mateix dispositiu. Comencem baixant l’instal·lador des d’aci UniFi-Installer per a windows. El controlador de xarxa UniFi depen de Java per funcionar. 9.2.1 Procediment Definiu les VLAN i les subxarxes a pfSense Configureu DHCP a pfSense Configureu les regles del tallafoc a pfSense Mapar les VLAN al controlador UniFi Definiu els SSID WiFi al controlador UniFi Els tres primers passos els tenim en l’apartat de pfSense xarxes a configurar Privada VLAN 20 Pública VLAN 40 Video, No internet VLAN 30 La diferència entre aquests és que la xarxa pública pot accedir als recursos de la xarxa interna, Nextcloud, registrador de video, la publica soles a internet, i la de video soles a la seua xarxa … 9.2.2 Assigna les VLAN al controlador UniFi A continuació, anem a UniFi Controller per assignar les VLAN als SSID WiFi A la configuració, feu clic a Xarxes i després a Crea una xarxa nova. Seleccioneu VLAN Només per al propòsit, poseu un nom a la xarxa Privat i doneu-li l’etiqueta VLAN 20 com vam fer a pfSense crea wifi privat Deseu la xarxa i feu el mateix per definir la VLAN Pública Després de desar aquestes dues xarxes, es mostraran a la llista de xarxes; confirmeu que són correctes xarxes wifi 9.2.2.1 Definiu els SSID de WiFi a UniFi Controller A continuació, crearem els SSID de WiFi i els enllaçarem a les nostres VLAN Al menú Configuració, trieu Xarxes sense fil (just a sobre de Xarxes) i feu clic a Crea una xarxa sense fil nova Doneu el nom SSID “Privat” i configureu el xifratge com corresponga (p. ex. WPA-PSK) Feu clic a Opcions avançades per mostrar el camp de l’etiqueta VLAN i introduïu 20 perquè aquest SSID gestionarà el trànsit de la VLAN Privat. xarxa privat ssdi La publica i video, de la mateixa forma. Les imatges de configuració tretes de la fulla web de l’exemple, per no disposar de les antenes ni usuari registrat per al programa. 9.2.3 unifi-pfsense, on instal.lar el controlador de les antenes El programa de configuració, no el podem utilitzar fins que no tinguem les antenes i l’usuari registrat, es deixa pendent de configurar. Un script que instal·la el programari UniFi Controller a pfSense i altres sistemes FreeBSD repositori Perill, es trenca si pfSense està en MBR Aquest script destruirà un sistema BIOS heretat arrencat des d’un volum arrel ZFS amb format MBR Per evitar aquest problema, utilitzeu el mode UEFI si està disponible, feu servir particions GPT o utilitzeu un sistema de fitxers diferent de ZFS. L’actualitzador de pfSense eliminarà tot el que instal·leu que no haja arribat a pfSense, inclosos els paquets instal·lats per aquest script. Després d’actualitzar pfSense, haureu de tornar a executar aquest script per restaurar les dependències i el programari. Massa pegues. Ho instal·larem en una LXC soles per aquest proposit, una vegada configurades les antenes el programa soles ens fa falta per si volem fer auditories de les connexions, apagarem el contenidor, i el tindrem reservat per si es vol canviar la configuració. "]]
